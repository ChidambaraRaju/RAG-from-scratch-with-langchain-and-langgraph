{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04953c1b",
   "metadata": {},
   "source": [
    "### üéØ Module Overview\n",
    "This module covers everything you need to know about parsing and ingesting data for RAG systems, from basic text files to complex PDFs and databases. We'll use LangChain v0.3 and explore each technique with practical examples.\n",
    "\n",
    "Table of Contents\n",
    "\n",
    "- Introduction to Data Ingestion\n",
    "- Text Files (.txt)\n",
    "- PDF Documents\n",
    "- Microsoft Word Documents\n",
    "- CSV and Excel Files\n",
    "- JSON and Structured Data\n",
    "- Web Scraping\n",
    "- Databases (SQL)\n",
    "- Audio and Video Transcripts\n",
    "- Advanced Techniques\n",
    "- Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884961ac",
   "metadata": {},
   "source": [
    "### Introduction To Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a0c84f-a7b2-4d1e-8e43-8f6453531f94",
   "metadata": {},
   "source": [
    "**Data Ingestion: The First Step in RAG**\n",
    "\n",
    "Data ingestion is the process of loading, processing, and preparing your external data to be used in a Retrieval-Augmented Generation (RAG) system. The goal is to convert unstructured or semi-structured data from various sources (like text files, PDFs, websites, etc.) into a clean, standardized format that can be effectively used for retrieval.\n",
    "\n",
    "The typical ingestion pipeline involves three key steps:\n",
    "1.  **Loading**: Reading data from its source. LangChain provides a wide variety of `DocumentLoaders` for this purpose.\n",
    "2.  **Splitting**: Breaking down large documents into smaller, manageable chunks. This is crucial for embedding and retrieval, as it helps the system find more precise pieces of information.\n",
    "3.  **Storing**: Placing the processed chunks (often after converting them into numerical vectors, or *embeddings*) into a specialized database called a Vector Store, where they can be efficiently searched.\n",
    "\n",
    "This notebook focuses on the first two steps: **Loading** and **Splitting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d033c0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os: A standard Python library for interacting with the operating system, used here for file path and directory management.\n",
    "import os\n",
    "\n",
    "# typing: Provides support for type hints, which makes the code more readable and easier to debug.\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# pandas: A powerful data manipulation and analysis library, useful for handling structured data like from CSV or Excel files.\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64e6820b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up Completed!\n"
     ]
    }
   ],
   "source": [
    "# Document: The fundamental object in LangChain for representing a piece of text and its associated metadata.\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# TextSplitters: These are classes designed to break down long texts into smaller chunks.\n",
    "from langchain.text_splitter import(\n",
    "    RecursiveCharacterTextSplitter, # Recommended for general text.\n",
    "    CharacterTextSplitter,          # Splits based on a single character.\n",
    "    TokenTextSplitter               # Splits based on language model tokens.\n",
    ")\n",
    "print(\"Set up Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d151ce3",
   "metadata": {},
   "source": [
    "### Understanding Document Structure In Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a530f2f0-e69e-4e48-af7e-128a113d0623",
   "metadata": {},
   "source": [
    "At the heart of LangChain's data handling is the `Document` object. Think of it as a standardized container for your data. No matter the source‚Äîbe it a text file, a PDF page, or a database row‚ÄîLangChain loaders will transform it into a `Document`.\n",
    "\n",
    "A `Document` has two main components:\n",
    "\n",
    "1.  `page_content` (string): This holds the actual text content of the data chunk.\n",
    "2.  `metadata` (dictionary): This is a dictionary containing extra information about the content. Common metadata includes the source file, page number, author, etc. Metadata is incredibly powerful for filtering searches and providing context to the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e29ecf98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Structure\n",
      "Content :This is the main text content that will be embedded and searched.\n",
      "Metadata :{'source': 'example.txt', 'page': 1, 'author': 'Krish Naik', 'date_created': '2024-01-01', 'cutom_field': 'any_value'}\n",
      "\n",
      "üìù Metadata is crucial for:\n",
      "- Filtering search results\n",
      "- Tracking document sources\n",
      "- Providing context in responses\n",
      "- Debugging and auditing\n"
     ]
    }
   ],
   "source": [
    "## create a simple document\n",
    "doc=Document(\n",
    "    # page_content: This is the core text that will be used for embedding and retrieval.\n",
    "    page_content=\"This is the main text content that will be embedded and searched.\",\n",
    "    \n",
    "    # metadata: A dictionary to hold supplementary information about the content.\n",
    "    metadata={\n",
    "        \"source\":\"example.txt\",        # The original file or source of the document.\n",
    "        \"page\":1,                      # The page number, if applicable.\n",
    "        \"author\":\"Krish Naik\",        # The author or creator of the content.\n",
    "        \"date_created\":\"2024-01-01\",   # The creation date.\n",
    "        \"cutom_field\":\"any_value\"       # You can add any custom fields you need.\n",
    "\n",
    "    }\n",
    ")\n",
    "print(\"Document Structure\")\n",
    "\n",
    "print(f\"Content :{doc.page_content}\")\n",
    "print(f\"Metadata :{doc.metadata}\")\n",
    "\n",
    "# Why metadata matters:\n",
    "print(\"\\nüìù Metadata is crucial for:\")\n",
    "print(\"- Filtering search results\")\n",
    "print(\"- Tracking document sources\")\n",
    "print(\"- Providing context in responses\")\n",
    "print(\"- Debugging and auditing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "592d84f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify the type of our created object to confirm it's a LangChain Document.\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0543b7",
   "metadata": {},
   "source": [
    "### Text Files (.txt) - The Simplest Case {#2-text-files}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a975659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we'll set up a directory to store our sample data.\n",
    "import os\n",
    "\n",
    "# Create a directory named 'data/text_files'. \n",
    "# The 'exist_ok=True' argument prevents an error if the directory already exists.\n",
    "os.makedirs(\"data/text_files\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2b14b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sample text files created!\n"
     ]
    }
   ],
   "source": [
    "# Define the content for our two sample text files in a dictionary.\n",
    "sample_texts={\n",
    "    \"data/text_files/python_intro.txt\":\"\"\"Python Programming Introduction\n",
    "\n",
    "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
    "Created by Guido van Rossum and first released in 1991, Python has become one of the most popular\n",
    "programming languages in the world.\n",
    "\n",
    "Key Features:\n",
    "- Easy to learn and use\n",
    "- Extensive standard library\n",
    "- Cross-platform compatibility\n",
    "- Strong community support\n",
    "\n",
    "Python is widely used in web development, data science, artificial intelligence, and automation.\"\"\",\n",
    "    \n",
    "    \"data/text_files/machine_learning.txt\": \"\"\"Machine Learning Basics\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
    "from experience without being explicitly programmed. It focuses on developing computer programs\n",
    "that can access data and use it to learn for themselves.\n",
    "\n",
    "Types of Machine Learning:\n",
    "1. Supervised Learning: Learning with labeled data\n",
    "2. Unsupervised Learning: Finding patterns in unlabeled data\n",
    "3. Reinforcement Learning: Learning through rewards and penalties\n",
    "\n",
    "Applications include image recognition, speech processing, and recommendation systems\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "}\n",
    "\n",
    "# Loop through the dictionary to create and write to each file.\n",
    "for filepath,content in sample_texts.items():\n",
    "    # Open the file in write mode ('w') with UTF-8 encoding to handle a wide range of characters.\n",
    "    with open(filepath,'w',encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"‚úÖ Sample text files created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c552f4e",
   "metadata": {},
   "source": [
    "### TextLoader: Read a Single File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d009b0b4-6a06-444a-8d14-b1539420970a",
   "metadata": {},
   "source": [
    "The `TextLoader` is one of the simplest document loaders. It's designed to read a single text file and load its entire content into one `Document` object.\n",
    "\n",
    "- **Input**: The file path to a single `.txt` file.\n",
    "- **Output**: A list containing a single `Document`. The `page_content` will be the full text of the file, and the `metadata` will automatically include the `source` (the file path)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c875d854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Loaded 1 document\n",
      "Content preview: Python Programming Introduction\n",
      "\n",
      "Python is a high-level, interpreted programming language known for ...\n",
      "Metadata: {'source': 'data/text_files/python_intro.txt'}\n"
     ]
    }
   ],
   "source": [
    "# Note: It's good practice to import from langchain_community for loaders and other integrations.\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "## Loading a single text file\n",
    "# Instantiate the loader with the path to the file and specify the encoding.\n",
    "loader=TextLoader(\"data/text_files/python_intro.txt\", encoding=\"utf-8\")\n",
    "\n",
    "# The .load() method reads the file and returns a list of Document objects.\n",
    "documents=loader.load()\n",
    "\n",
    "print(f\"üìÑ Loaded {len(documents)} document\")\n",
    "print(f\"Content preview: {documents[0].page_content[:100]}...\")\n",
    "print(f\"Metadata: {documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79596d23",
   "metadata": {},
   "source": [
    "### DirectoryLoader: Read Multiple Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5194098-b80c-4e89-8b43-b1d62054ff1a",
   "metadata": {},
   "source": [
    "When you need to load all files from a directory, `DirectoryLoader` is the tool to use. It's a convenient wrapper that can apply a specific loader (like `TextLoader`) to every file in a directory that matches a certain pattern.\n",
    "\n",
    "- **Input**: A directory path, a `glob` pattern to select files (e.g., `\"*.txt\"`), and a `loader_cls` specifying which loader to use for each file.\n",
    "- **Output**: A list containing one `Document` per file found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d625d3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 132.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÅ Loaded 2 documents\n",
      "\n",
      "Document 1:\n",
      "  Source: data\\text_files\\machine_learning.txt\n",
      "  Length: 575 characters\n",
      "\n",
      "Document 2:\n",
      "  Source: data\\text_files\\python_intro.txt\n",
      "  Length: 489 characters\n",
      "\n",
      "üìä DirectoryLoader Characteristics:\n",
      "‚úÖ Advantages:\n",
      "  - Loads multiple files at once\n",
      "  - Supports glob patterns\n",
      "  - Progress tracking\n",
      "  - Recursive directory scanning\n",
      "\n",
      "‚ùå Disadvantages:\n",
      "  - All files must be same type\n",
      "  - Limited error handling per file\n",
      "  - Can be memory intensive for large directories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "## load all the text files from the directory\n",
    "dir_loader=DirectoryLoader(\n",
    "    # The path to the directory containing the files.\n",
    "    \"data/text_files\",\n",
    "    \n",
    "    # A glob pattern to select which files to load. \"**/*.txt\" means all files ending with .txt in this directory and any subdirectories.\n",
    "    glob=\"**/*.txt\",\n",
    "    \n",
    "    # The specific loader class to use for each file found. Here, we use TextLoader.\n",
    "    loader_cls= TextLoader,\n",
    "    \n",
    "    # Keyword arguments to pass to the loader_cls during instantiation. We pass the encoding for TextLoader.\n",
    "    loader_kwargs={'encoding': 'utf-8'},\n",
    "    \n",
    "    # If True, displays a progress bar to show loading progress.\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "# The .load() method iterates through the directory, applies the loader to each matching file, and collects the results.\n",
    "documents=dir_loader.load()\n",
    "\n",
    "print(f\"\\nüìÅ Loaded {len(documents)} documents\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"  Source: {doc.metadata['source']}\")\n",
    "    print(f\"  Length: {len(doc.page_content)} characters\")\n",
    "\n",
    "# üìä Analysis\n",
    "print(\"\\nüìä DirectoryLoader Characteristics:\")\n",
    "print(\"‚úÖ Advantages:\")\n",
    "print(\"  - Loads multiple files at once\")\n",
    "print(\"  - Supports glob patterns\")\n",
    "print(\"  - Progress tracking\")\n",
    "print(\"  - Recursive directory scanning\")\n",
    "\n",
    "print(\"\\n‚ùå Disadvantages:\")\n",
    "print(\"  - All files must be same type\")\n",
    "print(\"  - Limited error handling per file\")\n",
    "print(\"  - Can be memory intensive for large directories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c621c75",
   "metadata": {},
   "source": [
    "### Text Splitting Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d297a7e-128a-4c28-98e3-5f0535e5d311",
   "metadata": {},
   "source": [
    "**Why Do We Need to Split Text?**\n",
    "\n",
    "Large language models (LLMs) have a limited **context window**, which is the maximum number of tokens they can process at once. If a document is larger than this window, it cannot be processed in its entirety. Furthermore, for effective retrieval in RAG, we want to find and use only the most relevant parts of a document, not the whole thing.\n",
    "\n",
    "Splitting a large document into smaller chunks addresses both issues:\n",
    "1.  **Fits within the Context Window**: Ensures each piece of text sent to the LLM is of a manageable size.\n",
    "2.  **Improves Retrieval Accuracy**: By embedding smaller, more focused chunks, the retrieval system can find pieces of text that are semantically very close to the user's query. Retrieving a whole book chapter about Python when the user asks a specific question about `for` loops is far less effective than retrieving a small paragraph that directly addresses it.\n",
    "\n",
    "The goal of text splitting is to create chunks that are **semantically meaningful** and of an appropriate size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "deaf6508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'data\\\\text_files\\\\machine_learning.txt'}, page_content='Machine Learning Basics\\n\\nMachine learning is a subset of artificial intelligence that enables systems to learn and improve\\nfrom experience without being explicitly programmed. It focuses on developing computer programs\\nthat can access data and use it to learn for themselves.\\n\\nTypes of Machine Learning:\\n1. Supervised Learning: Learning with labeled data\\n2. Unsupervised Learning: Finding patterns in unlabeled data\\n3. Reinforcement Learning: Learning through rewards and penalties\\n\\nApplications include image recognition, speech processing, and recommendation systems\\n\\n\\n    '), Document(metadata={'source': 'data\\\\text_files\\\\python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]\n"
     ]
    }
   ],
   "source": [
    "### Different text splitting strategies\n",
    "from langchain.text_splitter import (\n",
    "    CharacterTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    TokenTextSplitter\n",
    ")\n",
    "# The 'documents' variable currently holds two Document objects, one for each file we loaded.\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d098593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine Learning Basics\\n\\nMachine learning is a subset of artificial intelligence that enables systems to learn and improve\\nfrom experience without being explicitly programmed. It focuses on developing computer programs\\nthat can access data and use it to learn for themselves.\\n\\nTypes of Machine Learning:\\n1. Supervised Learning: Learning with labeled data\\n2. Unsupervised Learning: Finding patterns in unlabeled data\\n3. Reinforcement Learning: Learning through rewards and penalties\\n\\nApplications include image recognition, speech processing, and recommendation systems\\n\\n\\n    '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll select the first document (about machine learning) to demonstrate splitting.\n",
    "text=documents[0].page_content\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d7876a-74d7-4630-b30a-110f0e08c02c",
   "metadata": {},
   "source": [
    "#### Method 1: `CharacterTextSplitter`\n",
    "\n",
    "This is the simplest splitting method. It splits text based on a specified single character `separator` and then groups the resulting parts into chunks of a certain `chunk_size`.\n",
    "\n",
    "- **Pro**: Very straightforward and predictable.\n",
    "- **Con**: It doesn't respect the semantic structure of the text. It might split a sentence or even a word right in the middle if the chunk size limit is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e982f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1Ô∏è‚É£ CHARACTER TEXT SPLITTER (by space)\n",
      "Created 3 chunks\n",
      "First chunk: Machine Learning Basics\n",
      "\n",
      "Machine learning is a subset of artificial intelligence that enables system...\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Character-based splitting, using a space character as the separator.\n",
    "print(\"1Ô∏è‚É£ CHARACTER TEXT SPLITTER (by space)\")\n",
    "char_splitter = CharacterTextSplitter(\n",
    "    separator=\" \",          # The character to split the text on.\n",
    "    chunk_size=200,         # The maximum size of each chunk (in characters).\n",
    "    chunk_overlap=20,       # The number of characters to overlap between consecutive chunks. This helps maintain context.\n",
    "    length_function=len     # The function used to measure the length of a chunk (default is len).\n",
    ")\n",
    "\n",
    "# The .split_text() method performs the splitting.\n",
    "char_chunks=char_splitter.split_text(text)\n",
    "print(f\"Created {len(char_chunks)} chunks\")\n",
    "print(f\"First chunk: {char_chunks[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d707e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning Basics\n",
      "\n",
      "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
      "from experience without being explicitly programmed. It focuses on developing\n",
      "------------------\n",
      "on developing computer programs\n",
      "that can access data and use it to learn for themselves.\n",
      "\n",
      "Types of Machine Learning:\n",
      "1. Supervised Learning: Learning with labeled data\n",
      "2. Unsupervised Learning:\n"
     ]
    }
   ],
   "source": [
    "# Notice the overlap between the end of the first chunk and the start of the second.\n",
    "print(char_chunks[0])\n",
    "print(\"------------------\")\n",
    "print(char_chunks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d98de149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1Ô∏è‚É£ CHARACTER TEXT SPLITTER (by newline)\n",
      "Created 4 chunks\n",
      "First chunk: Machine Learning Basics\n",
      "Machine learning is a subset of artificial intelligence that enables systems...\n"
     ]
    }
   ],
   "source": [
    "# Let's try again with a more logical separator for this text: a newline character.\n",
    "print(\"1Ô∏è‚É£ CHARACTER TEXT SPLITTER (by newline)\")\n",
    "char_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",  # Split on newlines, which often separate paragraphs or logical blocks.\n",
    "    chunk_size=200,  \n",
    "    chunk_overlap=20,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "char_chunks=char_splitter.split_text(text)\n",
    "print(f\"Created {len(char_chunks)} chunks\")\n",
    "print(f\"First chunk: {char_chunks[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03d25bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning Basics\n",
      "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
      "-------------\n",
      "from experience without being explicitly programmed. It focuses on developing computer programs\n",
      "that can access data and use it to learn for themselves.\n",
      "Types of Machine Learning:\n",
      "-------------\n",
      "1. Supervised Learning: Learning with labeled data\n",
      "2. Unsupervised Learning: Finding patterns in unlabeled data\n",
      "3. Reinforcement Learning: Learning through rewards and penalties\n"
     ]
    }
   ],
   "source": [
    "# The chunks now look more coherent as they are split by paragraphs.\n",
    "print(char_chunks[0])\n",
    "print(\"-------------\")\n",
    "print(char_chunks[1])\n",
    "print(\"-------------\")\n",
    "print(char_chunks[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f99849-5e72-463d-82c8-7c858b1ab6b9",
   "metadata": {},
   "source": [
    "#### Method 2: `RecursiveCharacterTextSplitter` (Recommended)\n",
    "\n",
    "This is the most recommended and versatile text splitter. It works by trying to split the text using a list of separators in a specific order. It starts with the first separator (e.g., `\"\\n\\n\"` for paragraphs) and, if the resulting chunks are still too large, it moves to the next separator (e.g., `\"\\n\"` for lines), and so on.\n",
    "\n",
    "- **Why it's better**: It tries to keep semantically related pieces of text together as long as possible (e.g., paragraphs, then sentences). This hierarchical splitting approach generally results in more coherent and meaningful chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc2177d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2Ô∏è‚É£ RECURSIVE CHARACTER TEXT SPLITTER\n",
      "Created 4 chunks\n",
      "First chunk: Machine Learning Basics\n",
      "\n",
      "Machine learning is a subset of artificial intelligence that enables system...\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Recursive character splitting (RECOMMENDED)\n",
    "print(\"\\n2Ô∏è‚É£ RECURSIVE CHARACTER TEXT SPLITTER\")\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    # A list of characters to try splitting on, in order of preference.\n",
    "    # It will try to split by paragraph (\\n\\n), then by line (\\n), then by space (' '), and finally by character ('').\n",
    "    # The default is [\"\\n\\n\", \"\\n\", \" \", \"\"]. We are only using space here for a simple example.\n",
    "    separators=[\" \"],\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "recursive_chunks = recursive_splitter.split_text(text)\n",
    "print(f\"Created {len(recursive_chunks)} chunks\")\n",
    "print(f\"First chunk: {recursive_chunks[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a70073f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning Basics\n",
      "\n",
      "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
      "from experience without being explicitly programmed. It focuses on developing\n",
      "-----------------\n",
      "on developing computer programs\n",
      "that can access data and use it to learn for themselves.\n",
      "\n",
      "Types of Machine Learning:\n",
      "1. Supervised Learning: Learning with labeled data\n",
      "2. Unsupervised Learning:\n",
      "------------------\n",
      "Learning: Finding patterns in unlabeled data\n",
      "3. Reinforcement Learning: Learning through rewards and penalties\n",
      "\n",
      "Applications include image recognition, speech processing, and recommendation\n"
     ]
    }
   ],
   "source": [
    "print(recursive_chunks[0])\n",
    "print(\"-----------------\")\n",
    "print(recursive_chunks[1])\n",
    "print(\"------------------\")\n",
    "print(recursive_chunks[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96364077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simple text example - 4 chunks:\n",
      "\n",
      "Chunk 1: 'This is sentence one and it is quite long. This is sentence two and it is also'\n",
      "Chunk 2: 'two and it is also quite long. This is sentence three which is even longer than'\n",
      "\n",
      "Chunk 2: 'two and it is also quite long. This is sentence three which is even longer than'\n",
      "Chunk 3: 'is even longer than the others. This is sentence four. This is sentence five.'\n",
      "\n",
      "Chunk 3: 'is even longer than the others. This is sentence four. This is sentence five.'\n",
      "Chunk 4: 'is sentence five. This is sentence six.'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create text without natural break points like newlines to see how the splitter handles it.\n",
    "simple_text = \"This is sentence one and it is quite long. This is sentence two and it is also quite long. This is sentence three which is even longer than the others. This is sentence four. This is sentence five. This is sentence six.\"\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\" \"],  # For this example, we only allow splitting on spaces.\n",
    "    chunk_size=80,      # A smaller chunk size to demonstrate the splitting.\n",
    "    chunk_overlap=20,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(simple_text)\n",
    "\n",
    "print(f\"\\nSimple text example - {len(chunks)} chunks:\\n\")\n",
    "\n",
    "# Iterate through the chunks to show the content and the overlap between them.\n",
    "for i in range(len(chunks) - 1):\n",
    "    print(f\"Chunk {i+1}: '{chunks[i]}'\")\n",
    "    print(f\"Chunk {i+2}: '{chunks[i+1]}'\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8471b072-f8b1-419b-b9f1-f050b1d3d65b",
   "metadata": {},
   "source": [
    "#### Method 3: `TokenTextSplitter`\n",
    "\n",
    "Language models don't see characters; they see **tokens**. A token is a common sequence of characters in the text. For example, the word \"Apple\" might be one token, while a complex word like \"RAG\" might be broken into three tokens: \"R\", \"A\", \"G\".\n",
    "\n",
    "`TokenTextSplitter` splits the text based on the number of tokens. This is the most accurate way to split text if your primary concern is staying within the token limit of a model.\n",
    "\n",
    "- **Pro**: Aligns perfectly with how language models process text and their context limits.\n",
    "- **Con**: Can be slightly slower than character-based methods because it needs to tokenize the text first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b368869d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3Ô∏è‚É£ TOKEN TEXT SPLITTER\n",
      "Created 3 chunks\n",
      "First chunk: Machine Learning Basics\n",
      "\n",
      "Machine learning is a subset of artificial intelligence that enables system...\n"
     ]
    }
   ],
   "source": [
    "# Method 3: Token-based splitting\n",
    "print(\"\\n3Ô∏è‚É£ TOKEN TEXT SPLITTER\")\n",
    "token_splitter = TokenTextSplitter(\n",
    "    # Note: The chunk_size and chunk_overlap now refer to the number of tokens, not characters.\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=10\n",
    "    # This splitter uses the tiktoken library by default, which is the tokenizer used by OpenAI models.\n",
    ")\n",
    "\n",
    "token_chunks = token_splitter.split_text(text)\n",
    "print(f\"Created {len(token_chunks)} chunks\")\n",
    "print(f\"First chunk: {token_chunks[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c607dd",
   "metadata": {},
   "source": [
    "### üìä Text Splitting Methods Comparison\n",
    "\n",
    "---\n",
    "\n",
    "#### CharacterTextSplitter\n",
    "* ‚úÖ **Simple and predictable**\n",
    "* ‚úÖ **Good for structured text**\n",
    "* ‚ùå **May break mid-sentence**\n",
    "* **Use when**: Text has clear, consistent delimiters (like CSV data).\n",
    "\n",
    "---\n",
    "\n",
    "#### RecursiveCharacterTextSplitter\n",
    "* ‚úÖ **Respects text structure** by trying multiple separators.\n",
    "* ‚úÖ **Best general-purpose splitter.**\n",
    "* ‚ùå **Slightly more complex** to configure.\n",
    "* **Use when**: This is your default choice for most unstructured text like articles or books.\n",
    "\n",
    "---\n",
    "\n",
    "#### TokenTextSplitter\n",
    "* ‚úÖ **Respects model token limits** perfectly.\n",
    "* ‚úÖ **Most accurate for embeddings** as it aligns with how the model \"sees\" the text.\n",
    "* ‚ùå **Slower** than character-based methods.\n",
    "* **Use when**: It's critical to control the exact number of tokens per chunk for a specific model.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0992f5",
   "metadata": {},
   "source": [
    "### üîë Key Takeaways\n",
    "\n",
    "* **The `Document` Object is Fundamental**: All data in LangChain is standardized into `Document` objects. Each `Document` contains the main text (`page_content`) and extra information (`metadata`) like the source, which is crucial for filtering and context.\n",
    "* **Loaders for Every Source**: LangChain provides `DocumentLoaders` to handle various data sources. We saw how `TextLoader` handles a single file and `DirectoryLoader` efficiently loads all matching files in a folder.\n",
    "* **Splitting is Non-Negotiable for RAG**: Large documents must be split into smaller chunks. This is critical to fit the text into a model's limited context window and to allow the retrieval system to find highly relevant, specific pieces of information.\n",
    "* **Choose the Right Splitting Strategy**: While there are several methods, `RecursiveCharacterTextSplitter` is the recommended general-purpose choice because it intelligently tries to preserve the structure of the text (paragraphs, sentences). For ultimate precision related to a model's limits, `TokenTextSplitter` is the most accurate.\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ultimate RAG Bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
