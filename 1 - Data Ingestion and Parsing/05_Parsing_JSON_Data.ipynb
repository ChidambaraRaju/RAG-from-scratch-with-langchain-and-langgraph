{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a987e0f2-b8d4-4b52-9c9e-5b23d9a1012a",
   "metadata": {},
   "source": [
    "### 📖 Where We Are\n",
    "\n",
    "**So far**, our journey has taken us through various data formats:\n",
    "1.  **Notebook 1-3**: Handled unstructured and semi-structured text, PDFs, and Word documents.\n",
    "2.  **Notebook 4**: Dove into structured (tabular) data with CSV and Excel files, focusing on converting rows into meaningful text.\n",
    "\n",
    "**In this notebook**, we'll tackle **JSON (JavaScript Object Notation)**, the most common format for semi-structured data used in APIs and web services. We will learn how to parse its nested, hierarchical structure to extract meaningful information and also touch upon the efficient JSON Lines (`.jsonl`) format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc76ced2",
   "metadata": {},
   "source": [
    "### 1. JSON Parsing and Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a0c84f-a7b2-4d1e-8e43-8f6453531f94",
   "metadata": {},
   "source": [
    "JSON data is semi-structured, meaning it doesn't have a rigid schema like a database table but does have a clear hierarchical structure of keys and values. It can contain nested objects and lists, making it incredibly flexible but also challenging to parse for RAG.\n",
    "\n",
    "**Analogy**: Think of a nested JSON file as a detailed report about a company. The main report (the top-level object) has sections for 'Employees' and 'Departments'. The 'Employees' section is a list, where each item is a detailed file on a single employee, complete with their own sub-sections for 'Skills' and 'Projects'. Our job is to act as a researcher who can either grab an entire section (like all employee files) or intelligently pull specific information from multiple sections to create a comprehensive summary (like a single employee's profile with their department info).\n",
    "\n",
    "First, let's create our sample JSON data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf84111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json library is a standard Python library for working with JSON data.\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Create a directory for our sample files.\n",
    "os.makedirs(\"data/json_files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c260a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a complex, nested Python dictionary that we will use as our sample data.\n",
    "# It includes top-level keys, a list of objects ('employees'), and a dictionary of objects ('departments').\n",
    "json_data = {\n",
    "    \"company\": \"TechCorp\",\n",
    "    \"employees\": [\n",
    "        {\n",
    "            \"id\": 1,\n",
    "            \"name\": \"John Doe\",\n",
    "            \"role\": \"Software Engineer\",\n",
    "            \"skills\": [\"Python\", \"JavaScript\", \"React\"],\n",
    "            \"projects\": [\n",
    "                {\"name\": \"RAG System\", \"status\": \"In Progress\"},\n",
    "                {\"name\": \"Data Pipeline\", \"status\": \"Completed\"}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"id\": 2,\n",
    "            \"name\": \"Jane Smith\",\n",
    "            \"role\": \"Data Scientist\",\n",
    "            \"skills\": [\"Python\", \"Machine Learning\", \"SQL\"],\n",
    "            \"projects\": [\n",
    "                {\"name\": \"ML Model\", \"status\": \"In Progress\"},\n",
    "                {\"name\": \"Analytics Dashboard\", \"status\": \"Planning\"}\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"departments\": {\n",
    "        \"engineering\": {\n",
    "            \"head\": \"Mike Johnson\",\n",
    "            \"budget\": 1000000,\n",
    "            \"team_size\": 25\n",
    "        },\n",
    "        \"data_science\": {\n",
    "            \"head\": \"Sarah Williams\",\n",
    "            \"budget\": 750000,\n",
    "            \"team_size\": 15\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0ecc521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize the Python dictionary and save it to a .json file.\n",
    "# `json.dump` writes the object to a file.\n",
    "# `indent=2` makes the file human-readable with nice formatting.\n",
    "with open('data/json_files/company_data.json', 'w') as f:\n",
    "    json.dump(json_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e7c5ab-66a8-4c12-9e3f-8c3a51f0c2a1",
   "metadata": {},
   "source": [
    "### Understanding JSON Lines (`.jsonl`)\n",
    "\n",
    "Besides standard JSON, another common format is **JSON Lines**. In a `.jsonl` file, each line is a completely separate, valid JSON object. This format is highly efficient for streaming data, like application logs or event data, because you can process the file line by line without loading the entire, potentially massive, file into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e89f588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data for a JSON Lines file. It's a list of dictionaries.\n",
    "jsonl_data = [\n",
    "    {\"timestamp\": \"2024-01-01\", \"event\": \"user_login\", \"user_id\": 123},\n",
    "    {\"timestamp\": \"2024-01-01\", \"event\": \"page_view\", \"user_id\": 123, \"page\": \"/home\"},\n",
    "    {\"timestamp\": \"2024-01-01\", \"event\": \"purchase\", \"user_id\": 123, \"amount\": 99.99}\n",
    "]\n",
    "\n",
    "# To write a .jsonl file, we iterate through our list.\n",
    "with open('data/json_files/events.jsonl', 'w') as f:\n",
    "    for item in jsonl_data:\n",
    "        # `json.dumps` converts a single Python object to a JSON string.\n",
    "        # We write each JSON string followed by a newline character.\n",
    "        f.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be462888",
   "metadata": {},
   "source": [
    "## 2. JSON Processing Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f516d25e-38e8-4679-b883-7313a968a356",
   "metadata": {},
   "source": [
    "### Method 1: `JSONLoader` with `jq` Schema\n",
    "\n",
    "The `JSONLoader` in LangChain is extremely powerful because it integrates with `jq`, a command-line tool for processing JSON data. You provide a `jq` query (`jq_schema`) to specify exactly which parts of the JSON you want to extract into `Document` objects. This is perfect for pulling out all items from a list within a larger JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32a1f7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1️⃣ JSONLoader - Extracting specific fields with a jq schema\n",
      "Loaded 2 employee documents\n",
      "First employee's content: {\"id\": 1, \"name\": \"John Doe\", \"role\": \"Software Engineer\", \"skills\": [\"Python\", \"JavaScript\", \"React\"], \"projects\": [{\"name\": \"RAG System\", \"status\": \"In Progress\"}, {\"name\": \"Data Pipeline\", \"status\"...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "import json\n",
    "\n",
    "print(\"1️⃣ JSONLoader - Extracting specific fields with a jq schema\")\n",
    "\n",
    "# Initialize the loader to extract employee information.\n",
    "employee_loader = JSONLoader(\n",
    "    file_path='data/json_files/company_data.json',\n",
    "    # This jq schema says: 'Access the root object (.), find the key `employees`, and then iterate over each item in that array (`[]`).'\n",
    "    jq_schema='.employees[]',\n",
    "    # We set text_content=False to make the page_content of each Document the raw JSON object itself, not just its text values.\n",
    "    text_content=False\n",
    ")\n",
    "\n",
    "employee_docs = employee_loader.load()\n",
    "print(f\"Loaded {len(employee_docs)} employee documents\")\n",
    "print(f\"First employee's content: {employee_docs[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f99849-5e72-463d-82c8-7c858b1ab6b9",
   "metadata": {},
   "source": [
    "### Method 2: Custom JSON Processing (Intelligent Approach)\n",
    "\n",
    "While `JSONLoader` is great for extraction, a custom function is often needed when you want to:\n",
    "1.  **Combine** data from different nested levels into a single document.\n",
    "2.  **Format** the output into a more readable, natural language string.\n",
    "3.  Create **highly specific metadata** based on the data's content.\n",
    "\n",
    "Here, we'll create a function that builds a detailed, human-readable profile for each employee, including their projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0ef19a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2️⃣ Custom JSON Processing\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "print(\"\\n2️⃣ Custom JSON Processing\")\n",
    "\n",
    "def process_json_intelligently(filepath: str) -> List[Document]:\n",
    "    \"\"\"Processes a complex JSON file, creating a formatted Document for each employee.\"\"\"\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    documents = []\n",
    "    \n",
    "    # Iterate through each employee object in the 'employees' list.\n",
    "    for emp in data.get('employees', []):\n",
    "        # Use an f-string to build a clean, readable profile for the page_content.\n",
    "        content = f\"\"\"Employee Profile:\n",
    "        Name: {emp['name']}\n",
    "        Role: {emp['role']}\n",
    "        Skills: {', '.join(emp['skills'])}\n",
    "\\n        Projects:\"\"\"\n",
    "        # Nest a loop to process the 'projects' list for the current employee.\n",
    "        for proj in emp.get('projects', []):\n",
    "            content += f\"\\n- {proj['name']} (Status: {proj['status']})\"\n",
    "        \n",
    "        # Create a Document with the formatted content and rich metadata.\n",
    "        doc = Document(\n",
    "            page_content=content,\n",
    "            metadata={\n",
    "                'source': filepath,\n",
    "                'data_type': 'employee_profile',\n",
    "                'employee_id': emp['id'],\n",
    "                'employee_name': emp['name'],\n",
    "                'role': emp['role']\n",
    "            }\n",
    "        )\n",
    "        documents.append(doc)\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b3b7c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'data/json_files/company_data.json', 'data_type': 'employee_profile', 'employee_id': 1, 'employee_name': 'John Doe', 'role': 'Software Engineer'}, page_content='Employee Profile:\\n        Name: John Doe\\n        Role: Software Engineer\\n        Skills: Python, JavaScript, React\\n\\n        Projects:\\n- RAG System (Status: In Progress)\\n- Data Pipeline (Status: Completed)')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run our custom function and inspect the first resulting Document.\n",
    "intelligent_json_docs = process_json_intelligently(\"data/json_files/company_data.json\")\n",
    "intelligent_json_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e3b0a2-ad44-78ca-93d7-e44d371ba53d",
   "metadata": {},
   "source": [
    "### 📊 JSON Processing Strategy Comparison\n",
    "\n",
    "| Strategy | How it Works | Best For |\n",
    "| :--- | :--- | :--- |\n",
    "| **`JSONLoader`** | Uses a `jq` query to extract specific objects or values. | Quickly pulling out all items from a specific list within a large JSON file (e.g., all user comments from a product page JSON). |\n",
    "| **Custom Function** | Manually parses the JSON, allowing for complex logic, formatting, and data combination. | **Complex RAG scenarios**. Ideal for creating context-rich documents that combine information from multiple nested levels into a single, coherent text. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f280a82b-5813-4c90-99c5-0370483ba645",
   "metadata": {},
   "source": [
    "### 🔑 Key Takeaways\n",
    "\n",
    "* **JSON is Hierarchical**: The main challenge with JSON is navigating its nested structure. Your goal is to extract meaningful, self-contained \"sub-documents\" from the larger file.\n",
    "* **`jq` is Your Shortcut**: LangChain's `JSONLoader` with a `jq_schema` is a powerful and concise tool for extracting specific lists or objects. Learning basic `jq` syntax is a high-leverage skill for data processing.\n",
    "* **Custom Functions Offer Control**: For maximum flexibility, a custom Python function is the best approach. It allows you to combine data from different parts of the JSON tree, format the content for better readability by an LLM, and create precise metadata.\n",
    "* **Use JSON Lines for Streams**: The `.jsonl` format is highly efficient for large datasets of discrete records (like logs or events) because it can be processed one line at a time without loading the whole file into memory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ultimate RAG Bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
