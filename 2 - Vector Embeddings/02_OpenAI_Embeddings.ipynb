{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a987e0f2-b8d4-4b52-9c9e-5b23d9a1012a",
   "metadata": {},
   "source": [
    "### üìñ Where We Are\n",
    "\n",
    "**In the last notebook**, we learned the fundamentals of vector embeddings. We explored how they capture semantic meaning in numbers, how to measure their similarity using cosine similarity, and how to generate them using free, open-source models from Hugging Face that run on our own machine.\n",
    "\n",
    "**In this notebook**, we'll explore the other major category of embedding models: **proprietary, API-based models**, using OpenAI as our primary example. We'll learn how to use them with LangChain, compare the different models they offer, and build a simple yet powerful **semantic search** function from scratch to see embeddings in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bf6354",
   "metadata": {},
   "source": [
    "### 1. OpenAI Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a0c84f-a7b2-4d1e-8e43-8f6453531f94",
   "metadata": {},
   "source": [
    "While open-source models are fantastic for their accessibility and control, proprietary models offered by services like OpenAI often provide state-of-the-art performance. The main difference is that instead of running the model on your computer, you send your text to the provider's API and receive the embedding back. This requires an API key and usually involves a cost per use, but it offloads the computational heavy lifting.\n",
    "\n",
    "**Analogy: Home Cooking vs. a Michelin Star Restaurant üßë‚Äçüç≥**\n",
    "\n",
    "-   **Hugging Face (Local Models)** is like having a professional kitchen at home. You have full control, there's no cost per use (after buying the equipment), but you're responsible for the setup and computation.\n",
    "-   **OpenAI (API Models)** is like dining at a Michelin-star restaurant. You simply send your order (text), and a team of world-class chefs (the model) prepares a perfect dish (the embedding) for you. It's convenient and high-quality, but you pay for the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7067df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use the OpenAI API, we need to manage our API key securely.\n",
    "import os\n",
    "# `dotenv` is a library that helps load environment variables from a .env file.\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# This loads the variables from your .env file into the environment.\n",
    "# Make sure you have a .env file with OPENAI_API_KEY=\"sk-...\"\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def39c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain libraries automatically look for the OPENAI_API_KEY environment variable.\n",
    "# This line explicitly sets it, which is a good practice for clarity.\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee3d7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the LangChain wrapper for OpenAI's embedding models.\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Instantiate the embeddings model.\n",
    "# \"text-embedding-3-small\" is OpenAI's latest, most cost-effective, and highly performant model.\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835d1a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The interface is identical to the HuggingFaceEmbeddings wrapper.\n",
    "single_text = \"Langchain and Rag are amazing frameworks and projects to work on\"\n",
    "single_embeddings = embeddings.embed_query(single_text)\n",
    "\n",
    "# The 'text-embedding-3-small' model produces vectors with 1536 dimensions.\n",
    "print(f\"Vector dimensions: {len(single_embeddings)}\")\n",
    "print(f\"Sample values: {single_embeddings[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bcce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also embed multiple documents at once for efficiency.\n",
    "multiple_texts = [\n",
    "    \"Python is a programming language\",\n",
    "    \"LangChain is a framework for LLM applications\",\n",
    "    \"Embeddings convert text to numbers\",\n",
    "    \"Vectors can be compared for similarity\"\n",
    "]\n",
    "\n",
    "multiple_embeddings = embeddings.embed_documents(multiple_texts)\n",
    "\n",
    "print(f\"Number of embeddings: {len(multiple_embeddings)}\")\n",
    "print(f\"Dimension of each embedding: {len(multiple_embeddings[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e68d06-1ddf-44e2-9b26-a320c8c156c7",
   "metadata": {},
   "source": [
    "### üìä OpenAI Embedding Models Comparison\n",
    "\n",
    "OpenAI offers several embedding models, each with different capabilities, sizes, and costs.\n",
    "\n",
    "| Model | Dimensions | Cost / 1M Tokens | Description | Best For |\n",
    "| :--- | :---: | :---: | :--- | :--- |\n",
    "| **`text-embedding-3-small`** | 1536 | $0.02 | Good balance of performance and cost. | General purpose, cost-effective applications. |\n",
    "| **`text-embedding-3-large`** | 3072 | $0.13 | Highest quality and performance. | When accuracy is absolutely critical. |\n",
    "| **`text-embedding-ada-002`** | 1536 | $0.10 | Previous generation, still widely used. | Legacy applications or specific compatibility needs. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde1359e",
   "metadata": {},
   "source": [
    "### 2. Cosine Similarity With OpenAI Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f99849-5e72-463d-82c8-7c858b1ab6b9",
   "metadata": {},
   "source": [
    "The principles of vector similarity are universal. The same `cosine_similarity` function we used for Hugging Face embeddings works perfectly for OpenAI embeddings because, in the end, they are all just vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee08ac04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the same function from the previous notebook.\n",
    "import numpy as np\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_a = np.linalg.norm(vec1)\n",
    "    norm_b = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2360d942",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"A feline rested on the rug\", # Semantically very similar to the first\n",
    "    \"The dog played in the yard\", # Related, but different\n",
    "    \"I love programming in Python\" # Unrelated\n",
    "]\n",
    "\n",
    "# Generate embeddings for these sentences\n",
    "sentence_embeddings = embeddings.embed_documents(sentences)\n",
    "\n",
    "# Calculate the similarity between the two cat sentences.\n",
    "cat_similarity = cosine_similarity(sentence_embeddings[0], sentence_embeddings[1])\n",
    "print(f\"Similarity between 'cat' sentences: {cat_similarity:.4f}\")\n",
    "\n",
    "# Calculate the similarity between the cat and dog sentences.\n",
    "dog_similarity = cosine_similarity(sentence_embeddings[0], sentence_embeddings[2])\n",
    "print(f\"Similarity between 'cat' and 'dog' sentences: {dog_similarity:.4f}\")\n",
    "\n",
    "# Calculate the similarity between the cat and Python sentences.\n",
    "python_similarity = cosine_similarity(sentence_embeddings[0], sentence_embeddings[3])\n",
    "print(f\"Similarity between 'cat' and 'Python' sentences: {python_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f0a0e9-d3e6-4b5a-967a-1c7b2b0a1e38",
   "metadata": {},
   "source": [
    "### 3. Building a Simple Semantic Search\n",
    "\n",
    "Now we can put everything together to build the core component of a RAG system: **semantic search**. The goal is to take a user's query, compare it against a collection of documents, and retrieve the most relevant ones based on semantic meaning, not just keyword matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f3354a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query: str, documents: list[str], embeddings_model, top_k: int = 2):\n",
    "    \"\"\"Performs a simple semantic search.\n",
    "    \n",
    "    Args:\n",
    "        query: The user's search query.\n",
    "        documents: A list of documents to search through.\n",
    "        embeddings_model: The embedding model to use.\n",
    "        top_k: The number of top results to return.\n",
    "    \"\"\"\n",
    "    # 1. Embed the user's query.\n",
    "    query_embedding = embeddings_model.embed_query(query)\n",
    "    \n",
    "    # 2. Embed all the documents.\n",
    "    doc_embeddings = embeddings_model.embed_documents(documents)\n",
    "    \n",
    "    # 3. Calculate the cosine similarity between the query and each document.\n",
    "    similarities = []\n",
    "    for i, doc_emb in enumerate(doc_embeddings):\n",
    "        similarity = cosine_similarity(query_embedding, doc_emb)\n",
    "        similarities.append((similarity, documents[i]))\n",
    "    \n",
    "    # 4. Sort the documents by similarity score in descending order.\n",
    "    similarities.sort(key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    # 5. Return the top_k most similar documents.\n",
    "    return similarities[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9277712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our corpus of documents to search.\n",
    "documents = [\n",
    "    \"LangChain is a framework for developing applications powered by language models\",\n",
    "    \"Python is a high-level programming language\",\n",
    "    \"Machine learning is a subset of artificial intelligence\",\n",
    "    \"Embeddings convert text into numerical vectors\",\n",
    "    \"The weather today is sunny and warm\"\n",
    "]\n",
    "query = \"What is LangChain?\"\n",
    "\n",
    "# Perform the search.\n",
    "results = semantic_search(query, documents, embeddings)\n",
    "\n",
    "print(f\"\\nüîé Semantic Search Results for: '{query}'\")\n",
    "for score, doc in results:\n",
    "    print(f\"Score: {score:.4f} | Document: {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f280a82b-5813-4c90-99c5-0370483ba645",
   "metadata": {},
   "source": [
    "### üîë Key Takeaways\n",
    "\n",
    "* **API vs. Local Models**: OpenAI provides high-performance embedding models via a simple API. This contrasts with local models (like from Hugging Face) which you run on your own hardware. API models are convenient but have associated costs.\n",
    "* **Consistent LangChain Interface**: A major advantage of LangChain is its consistent API. The `.embed_query()` and `.embed_documents()` methods work the same way for `OpenAIEmbeddings` as they do for `HuggingFaceEmbeddings`.\n",
    "* **Semantic Search is the Goal**: The primary application of embeddings in RAG is semantic search. This process involves embedding a query and a set of documents, then using a similarity metric (like cosine similarity) to find and rank the most relevant documents.\n",
    "* **Cost and Performance Trade-offs**: When using proprietary models, it's important to choose the right one for your task. Models like `text-embedding-3-small` offer a great balance of cost and performance, while `text-embedding-3-large` provides the highest quality for more demanding applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ultimate RAG Bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
