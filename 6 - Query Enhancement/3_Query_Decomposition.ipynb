{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a987e0f2-b8d4-4b52-9c9e-5b23d9a1012a",
   "metadata": {},
   "source": [
    "### ðŸ“– Where We Are\n",
    "\n",
    "**In the last notebook**, we explored **Query Expansion**, a technique to make a simple user query more comprehensive to improve retrieval recall.\n",
    "\n",
    "**In this notebook**, we'll tackle the opposite problem: what to do when a user's query is too **complex**. We'll learn about **Query Decomposition**, a powerful strategy where we use an LLM to break down a single, multi-part question into several simpler sub-questions. This allows our RAG system to find focused, high-quality context for each part of the original query, leading to more complete and accurate answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e7d68c",
   "metadata": {},
   "source": [
    "### 1. What is Query Decomposition?\n",
    "\n",
    "Query decomposition is the process of taking a complex, multi-part question and breaking it into simpler, atomic sub-questions that can each be retrieved and answered individually. The final answers are then combined to address the original, complex query.\n",
    "\n",
    "#### âœ… Why Use Query Decomposition?\n",
    "\n",
    "- **Improves Retrieval**: A single vector search for a complex query (e.g., \"compare A and B\") may not find the best documents for either A or B individually.\n",
    "- **Reduces Missed Information**: It ensures that every part of the user's question is addressed.\n",
    "- **Enables Multi-Hop Reasoning**: It allows the system to gather facts from different documents and combine them to answer a question that isn't explicitly answered in any single chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b23a442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard LangChain and Environment Imports ---\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f43149d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API keys from .env file\n",
    "load_dotenv()\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "llm = init_chat_model(model=\"groq:gemma2-9b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76de0145",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\Ultimate RAG Bootcamp\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load and Index Documents ---\n",
    "# This follows the standard RAG indexing pipeline.\n",
    "loader = TextLoader(\"langchain_crewai_dataset.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(chunks, embedding)\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 4, \"lambda_mult\": 0.7})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a2a468-18e3-4610-ad44-783ca89369d7",
   "metadata": {},
   "source": [
    "### 2. Creating the Decomposition Chain\n",
    "This is the first step in our pipeline. We create a dedicated chain that uses an LLM to break down the user's complex question into a list of simpler sub-questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0af1982d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a specific prompt to instruct the LLM to act as a query decomposer.\n",
    "# The prompt guides the model to identify and separate the distinct parts of the original question.\n",
    "decomposition_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an expert at query decomposition. Your task is to break down a complex user question into 2-4 simpler, self-contained sub-questions. \n",
    "These sub-questions will be used to retrieve relevant documents.\n",
    "\n",
    "Question: \"{question}\"\n",
    "\n",
    "Sub-questions (one per line):\n",
    "\"\"\")\n",
    "\n",
    "# Create a simple LCEL chain for this decomposition task.\n",
    "decomposition_chain = decomposition_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c9797ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Query: How does LangChain use memory and agents compared to CrewAI?\n",
      "\n",
      "Decomposed Sub-Questions:\n",
      "1. What types of memory mechanisms does LangChain support?\n",
      "2. How do agents function in LangChain?\n",
      "3. What memory capabilities does CrewAI offer?\n",
      "4. How do agents work in CrewAI? \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's test the decomposition chain in isolation.\n",
    "query = \"How does LangChain use memory and agents compared to CrewAI?\"\n",
    "decomposed_questions = decomposition_chain.invoke({\"question\": query})\n",
    "\n",
    "print(\"Original Query:\", query)\n",
    "print(\"\\nDecomposed Sub-Questions:\")\n",
    "print(decomposed_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f99849-5e72-463d-82c8-7c858b1ab6b9",
   "metadata": {},
   "source": [
    "### 3. Building the Full RAG Pipeline\n",
    "Now we'll create a function that orchestrates the entire process:\n",
    "1.  Take the user's query.\n",
    "2.  Send it to the `decomposition_chain` to get the sub-questions.\n",
    "3.  For each sub-question, run a standard RAG process (retrieve -> generate answer).\n",
    "4.  Combine the answers into a final response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26c735b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need a standard QA chain. This chain will be called for EACH sub-question.\n",
    "qa_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Use the following context to answer the question.\n",
    "Context: {context}\n",
    "Question: {input}\n",
    "Answer:\"\"\")\n",
    "qa_chain = create_stuff_documents_chain(llm=llm, prompt=qa_prompt)\n",
    "\n",
    "# Now, create the function that ties everything together.\n",
    "def full_query_decomposition_rag_pipeline(user_query: str) -> str:\n",
    "    # 1. Decompose the original query into sub-questions.\n",
    "    sub_qs_text = decomposition_chain.invoke({\"question\": user_query})\n",
    "    # 2. Clean up the LLM's output to get a clean list of questions.\n",
    "    sub_questions = [q.strip() for q in sub_qs_text.strip().split(\"\\n\") if q.strip()]\n",
    "    \n",
    "    # 3. Initialize an empty list to store the results from each sub-query.\n",
    "    results = []\n",
    "    \n",
    "    print(f\"--- Decomposed into {len(sub_questions)} sub-questions ---\")\n",
    "    \n",
    "    # 4. Loop through each sub-question and execute a RAG chain for it.\n",
    "    for subq in sub_questions:\n",
    "        # a. Retrieve relevant documents for the *current* sub-question.\n",
    "        docs = retriever.invoke(subq)\n",
    "        # b. Use the QA chain to generate an answer for the sub-question.\n",
    "        result = qa_chain.invoke({\"input\": subq, \"context\": docs})\n",
    "        # c. Store the formatted Q&A pair.\n",
    "        results.append(f\"Sub-Question: {subq}\\nAnswer: {result}\")\n",
    "    \n",
    "    # 5. Combine all the individual answers into a single final string.\n",
    "    return \"\\n\\n\".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac50f32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Decomposed into 4 sub-questions ---\n",
      "\n",
      "âœ… Final Synthesized Answer:\n",
      "\n",
      "Sub-Question: 1. What types of memory mechanisms does LangChain utilize?\n",
      "Answer: LangChain uses memory modules like **ConversationBufferMemory** and **ConversationSummaryMemory**.  \n",
      "\n",
      "\n",
      "Sub-Question: 2. How do agents function within the LangChain framework?\n",
      "Answer: LangChain agents function using a **planner-executor model**.  Here's a breakdown:\n",
      "\n",
      "* **Planner:** The agent uses its understanding of the task and available tools to devise a sequence of actions (tool invocations) needed to achieve the goal. This involves reasoning, decision-making, and potentially branching logic based on the situation.\n",
      "* **Executor:** The agent then carries out the planned actions, interacting with the tools and processing their outputs.  \n",
      "\n",
      "Crucially, LangChain agents maintain **context-aware memory** across these steps. This means they remember previous actions, tool outputs, and other relevant information to make informed decisions in subsequent steps. \n",
      "\n",
      "\n",
      "Essentially, agents act like intelligent coordinators, leveraging LLMs to strategize and tools to execute, all while keeping track of the bigger picture. \n",
      "\n",
      "\n",
      "Sub-Question: 3. What memory capabilities does CrewAI offer?\n",
      "Answer: The provided context doesn't mention any specific memory capabilities of CrewAI. \n",
      "\n",
      "\n",
      "While it highlights features like streaming, parallel execution, and asynchronous tool invocation, it doesn't delve into details about its memory management or capacity. \n",
      "\n",
      "\n",
      "Sub-Question: 4.  How are agents implemented in CrewAI?\n",
      "Answer: Based on the provided context, CrewAI agents are implemented as semi-independent entities with defined roles:\n",
      "\n",
      "* **Purpose, Goal, and Tools:** Each agent has a specific purpose, a goal to achieve, and a set of tools to accomplish its tasks. \n",
      "* **Structured Workflows:**  Agents collaborate within structured workflows, forming crews where each member plays a distinct role (e.g., researcher, planner, executor).\n",
      "* **Framework for Task Management:**  The CrewAI framework ensures that agents remain focused on their tasks and contribute effectively to the overall objectives of the crew. \n",
      "\n",
      "\n",
      "Let me know if you have any other questions! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Run the Full Pipeline ---\n",
    "final_answer = full_query_decomposition_rag_pipeline(query)\n",
    "print(\"\\nâœ… Final Synthesized Answer:\\n\")\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f280a82b-5813-4c90-99c5-0370483ba645",
   "metadata": {},
   "source": [
    "### ðŸ”‘ Key Takeaways\n",
    "\n",
    "* **Decomposition for Complexity**: Query Decomposition is the ideal strategy when dealing with complex, multi-part user questions that a single search would struggle with.\n",
    "* **LLM as a Query Planner**: The core of this technique is using an LLM with a specific prompt to act as a \"query planner,\" breaking a large task into smaller, manageable sub-tasks (the sub-questions).\n",
    "* **Divide and Conquer**: The strategy follows a \"divide and conquer\" approach. By breaking down the problem, the RAG system can retrieve highly focused context for each sub-question, leading to more accurate individual answers.\n",
    "* **Orchestration is Key**: The implementation requires a controlling function or a more advanced framework (like LangGraph) to manage the multi-step process: decompose, loop through sub-queries, execute RAG for each, and synthesize the final result.\n",
    "* **Performance Trade-off**: This is a powerful but resource-intensive method. It involves multiple LLM calls and retrieval steps, leading to higher latency and cost compared to a single RAG query."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ultimate RAG Bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
