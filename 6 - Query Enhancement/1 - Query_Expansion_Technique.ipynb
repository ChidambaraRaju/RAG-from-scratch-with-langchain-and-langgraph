{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a987e0f2-b8d4-4b52-9c9e-5b23d9a1012a",
   "metadata": {},
   "source": [
    "### ðŸ“– Where We Are\n",
    "\n",
    "**In the previous section on Advanced Retrieval**, we focused on improving the *retrieval process itself* with techniques like Hybrid Search, Reranking, and MMR. We learned how to find and prioritize better documents from our vector store.\n",
    "\n",
    "**In this new section on Query Enhancement**, we shift our focus to the beginning of the pipeline: the **user's query**. We'll learn techniques to transform a user's initial, often simple, query into a more effective one *before* it ever hits the retriever. This notebook introduces the first and most powerful of these techniques: **Query Expansion**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26858fec",
   "metadata": {},
   "source": [
    "### 1. Query Expansion\n",
    "\n",
    "In a RAG pipeline, the quality of the query sent to the retriever determines the quality of the retrieved context, which in turn dictates the accuracy of the LLMâ€™s final answer. **Garbage in, garbage out.**\n",
    "\n",
    "Query Expansion is a technique that uses an LLM to reformulate a user's query to be more specific, comprehensive, and better aligned with the language in the source documents. This is especially useful when the original query is short, ambiguous, or uses different terminology than your documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b103023a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard LangChain and Environment Imports ---\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40f86e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API keys from .env file\n",
    "load_dotenv()\n",
    "llm = init_chat_model(\"groq:openai/gpt-oss-20b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "157104d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\Ultimate RAG Bootcamp\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load, Split, and Index Documents ---\n",
    "# This follows the standard RAG indexing pipeline.\n",
    "loader = TextLoader(\"langchain_crewai_dataset.txt\")\n",
    "raw_docs = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(raw_docs)\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(chunks, embedding_model)\n",
    "\n",
    "# Create our base retriever.\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a2a468-18e3-4610-ad44-783ca89369d7",
   "metadata": {},
   "source": [
    "### 2. Creating the Query Expansion Chain\n",
    "This is a small, dedicated chain whose only job is to take a user's query and use an LLM to make it better for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a17685e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a prompt that specifically instructs the LLM to expand the query.\n",
    "# It's crucial to tell it NOT to answer the question, but to improve it for a search.\n",
    "query_expansion_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an AI assistant. Your task is to expand the user's query to improve document retrieval.\n",
    "Add relevant synonyms, technical terms, and broader concepts related to the original query.\n",
    "Do not answer the query, only expand it.\n",
    "\n",
    "Original query: \"{query}\"\n",
    "\n",
    "Expanded query:\"\"\")\n",
    "\n",
    "# Create a simple LCEL chain for this task.\n",
    "query_expansion_chain = query_expansion_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fef26aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Query: Langchain memory\n",
      "\n",
      "Expanded Query: LangChain memory, LangChain memory modules, LangChain memory interface, LangChain memory adapters, LangChain memory providers, LangChain memory store, LangChain memory buffer, LangChain memory persistence, LangChain memory caching, LangChain memory management, LangChain memory architecture, LangChain memory implementation, LangChain memory component, LangChain memory classes, LangChain memory objects, LangChain memory state, LangChain memory context, LangChain memory usage patterns, LangChain memory optimization, LangChain memory plugin, LangChain memory chain, LangChain memory persistence layer, LangChain memory retrieval, LangChain memory system, LangChain memory repository, LangChain memory database, LangChain memory features, LangChain memory usage, LangChain memory management patterns, LangChain memory in conversational AI, LangChain memory in RAG, LangChain memory in chatbot, LangChain memory for LLM, LangChain memory in LLM applications, LangChain memory in stateful LLMs, LangChain memory context management, LangChain memory architecture design, LangChain memory design patterns, LangChain memory best practices, LangChain memory storage solutions, LangChain memory strategies, LangChain memory in large language models, LangChain memory integration, LangChain memory in language model pipelines, LangChain memory in AI systems, LangChain memory component design, LangChain memory implementation details, LangChain memory caching strategies, LangChain memory persistence strategies, LangChain memory state management, LangChain memory context switching, LangChain memory management for conversational agents, LangChain memory system architecture, LangChain memory in multi-turn dialogues, LangChain memory in AI workflows, LangChain memory for persistent state, LangChain memory for session state.\n"
     ]
    }
   ],
   "source": [
    "# Let's test the expansion chain in isolation.\n",
    "original_query = \"Langchain memory\"\n",
    "expanded_query = query_expansion_chain.invoke({\"query\": original_query})\n",
    "\n",
    "print(\"Original Query:\", original_query)\n",
    "print(\"\\nExpanded Query:\", expanded_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f99849-5e72-463d-82c8-7c858b1ab6b9",
   "metadata": {},
   "source": [
    "### 3. Building the Full RAG Pipeline with Expansion\n",
    "Now, we'll construct a complete RAG pipeline that incorporates our `query_expansion_chain`. We use a `RunnableMap` to manage the flow of data, ensuring the *original* query is used for the final answer while the *expanded* query is used for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66f1ef90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, define the final answering chain, which takes context and a question.\n",
    "answer_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on the context below.\n",
    "Context: {context}\n",
    "Question: {input}\n",
    "Answer:\"\"\")\n",
    "document_chain = create_stuff_documents_chain(llm=llm, prompt=answer_prompt)\n",
    "\n",
    "# Now, build the full pipeline using a RunnableMap.\n",
    "rag_pipeline = (\n",
    "    RunnableMap({\n",
    "        # The 'context' is generated by a sub-chain:\n",
    "        # 1. The original 'input' from the user is passed to the `query_expansion_chain`.\n",
    "        # 2. The result (the expanded query) is then used to invoke the retriever.\n",
    "        \"context\": lambda x: retriever.invoke(query_expansion_chain.invoke({\"query\": x[\"input\"]})),\n",
    "        # The 'input' is passed through directly from the original user query.\n",
    "        # This ensures the LLM answers the user's actual question.\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "    })\n",
    "    | document_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1bb6ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Answer:\n",
      " LangChain provides two builtâ€‘in memory types that help an LLM keep track of or condense prior dialogue:\n",
      "\n",
      "| Memory type | What it does |\n",
      "|-------------|--------------|\n",
      "| **ConversationBufferMemory** | Stores the entire conversation history (or a configurable buffer) so the model can refer to earlier turns. |\n",
      "| **ConversationSummaryMemory** | Keeps a running summary of the conversation, compressing long interactions into a concise recap that stays within token limits. |\n",
      "\n",
      "These modules let you choose whether to retain the full transcript or a summarized version when feeding context back into the LLM.\n"
     ]
    }
   ],
   "source": [
    "# Run the full pipeline with a simple query.\n",
    "query = {\"input\": \"What types of memory does LangChain support?\"}\n",
    "response = rag_pipeline.invoke(query)\n",
    "print(\"âœ… Answer:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e258db1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded Query:\n",
      " CrewAI agents, crew AI assistants, crew artificial intelligence agents, crew AI bots, crew AI software agents, crew AI autonomous agents, crew AI multiâ€‘agent systems, crew AI agent architecture, crew AI agent framework, crew AI agent development, crew AI agent integration, crew AI agent use cases, crew AI agent applications, crew AI agent technologies, crew AI agent software, crew AI agent platform, crew AI agent solutions, crew AI agent system, crew AI agent design, crew AI agent capabilities, crew AI agent benefits, crew AI agent challenges, crew AI agent examples, crew AI agent research, crew AI agent industry, crew AI agent deployment, crew AI agent training, crew AI agent evaluation, crew AI agent monitoring, crew AI agent performance, crew AI agent safety, crew AI agent reliability, crew AI agent ethics, crew AI agent regulation, crew AI agent policy, crew AI agent governance, crew AI in aviation, crew AI in maritime, crew AI in space missions, crew AI for crew management, crew AI for crew scheduling, crew AI for crew coordination, crew AI for workload optimization, crew AI for performance analytics, crew AI for decision support, crew resource management AI, crew scheduling AI, crew workload AI, crew automation AI, crew simulation AI, crew workflow AI, crew collaboration AI, crew decision support AI, AI agents, autonomous agents, intelligent agents, virtual assistants, digital assistants, bots, multiâ€‘agent systems, agentâ€‘based modeling, reinforcement learning, deep learning, natural language processing, humanâ€‘robot interaction, crew safety AI, crew performance AI, crew training AI, crew operations AI.\n",
      "\n",
      "âœ… Answer:\n",
      " CrewAI agents are semiâ€‘autonomous, roleâ€‘based agents that work together in a structured workflow.  \n",
      "Each agent is defined with a clear purpose, a specific goal, and a set of tools it can use. They have designated rolesâ€”such as researcher, planner, executorâ€”allowing them to operate independently yet collaboratively. The CrewAI framework orchestrates turnâ€‘taking and decisionâ€‘making, ensuring every agent stays on task and contributes meaningfully to the overall crew objective.\n"
     ]
    }
   ],
   "source": [
    "# Run the pipeline with a more ambiguous query.\n",
    "query = {\"input\": \"CrewAI agents?\"}\n",
    "\n",
    "# First, let's see how the query gets expanded.\n",
    "expanded_query = query_expansion_chain.invoke({\"query\": query['input']})\n",
    "print(\"Expanded Query:\\n\", expanded_query)\n",
    "\n",
    "# Now, run the full RAG pipeline.\n",
    "response = rag_pipeline.invoke(query)\n",
    "print(\"\\nâœ… Answer:\\n\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f280a82b-5813-4c90-99c5-0370483ba645",
   "metadata": {},
   "source": [
    "### ðŸ”‘ Key Takeaways\n",
    "\n",
    "* **Query Expansion Boosts Recall**: By reformulating a user's query to be more comprehensive, you significantly increase the chances of retrieving all relevant documents, especially for short or ambiguous queries.\n",
    "* **LLMs are Excellent Expanders**: Using an LLM with a specific prompt is a highly effective way to automatically generate synonyms, related technical terms, and clearer questions for your retriever.\n",
    "* **Isolate the Expansion Logic**: It's a best practice to create a dedicated chain for query expansion. This keeps your pipeline modular and easy to debug.\n",
    "* **`RunnableMap` for Complex Flows**: LangChain's `RunnableMap` is the perfect tool for orchestrating complex RAG pipelines. It allows you to run different chains in parallel and structure their outputs into the format needed for the next step (e.g., creating the `context` and `input` keys).\n",
    "* **Balance Performance and Cost**: Remember that query expansion adds an extra LLM call, which introduces latency and cost. It's a powerful tool that should be used when the improvement in retrieval quality justifies the trade-off."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ultimate RAG Bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
