{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a987e0f2-b8d4-4b52-9c9e-5b23d9a1012a",
   "metadata": {},
   "source": [
    "### ðŸ“– Where We Are\n",
    "\n",
    "**In the previous notebooks**, we explored query enhancement techniques like **Query Expansion** (making a query more detailed) and **Query Decomposition** (breaking a complex query into simpler parts).\n",
    "\n",
    "**In this notebook**, we'll learn about a third, fundamentally different technique: **HyDE (Hypothetical Document Embeddings)**. Instead of modifying the user's query text, HyDE generates a completely new, hypothetical document that answers the query. We then use the embedding of this *hypothetical answer* to find real, factual documents in our knowledge base. This is a powerful method for bridging the semantic gap between a question and its answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e23471",
   "metadata": {},
   "source": [
    "### 1. HyDE (Hypothetical Document Embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b320d771-48d6-4448-9c4c-31d77a83d973",
   "metadata": {},
   "source": [
    "ðŸ§  **What is HyDE?**\n",
    "\n",
    "HyDE is a retrieval technique where, instead of embedding the userâ€™s query directly, you first generate a hypothetical answer (document) to the query using an LLM. You then embed that hypothetical document to search your vector store. The core idea is that a hypothetical answer is likely to be much closer in the vector space to the real answer documents than the original question is.\n",
    "\n",
    "âž¡ï¸ **HyDE is most effective when:**\n",
    "\n",
    "1.  Queries are short or ambiguous.\n",
    "2.  There is a language mismatch between the query and the documents (e.g., question vs. statement).\n",
    "3.  You want to retrieve based on the *content of a likely answer*, not the words in the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38efa93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard LangChain and Environment Imports ---\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma, FAISS\n",
    "from langchain.chat_models import init_chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea67d371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Load and Chunk Documents ---\n",
    "# We'll use a Wikipedia loader to get a sample dataset about Steve Jobs.\n",
    "loader = WikipediaLoader(query=\"Steve Jobs\", load_max_docs=1)\n",
    "documents = loader.load()\n",
    "\n",
    "# Split the documents into manageable chunks.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "784942fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\Ultimate RAG Bootcamp\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Setup LLM and Vector Store ---\n",
    "load_dotenv()\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "llm = init_chat_model(\"groq:gemma2-9b-it\")\n",
    "\n",
    "# Create the vector store for our documents.\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "base_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f99849-5e72-463d-82c8-7c858b1ab6b9",
   "metadata": {},
   "source": [
    "### 3. Manual HyDE Implementation\n",
    "First, let's implement the HyDE logic manually to understand each step of the process clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f07abc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# This function takes a query and uses an LLM to generate a hypothetical answer.\n",
    "def generate_hypothetical_document(query: str):\n",
    "    # Create a prompt that instructs the LLM to generate a detailed, hypothetical answer.\n",
    "    template = \"\"\"Imagine you are an expert on the topic. Please write a concise, factual-sounding paragraph answering the following question:\n",
    "    Question: '{query}'\n",
    "    Hypothetical Answer:\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template=template)\n",
    "    \n",
    "    # Create a simple chain to pass the formatted prompt to the LLM.\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    \n",
    "    # Invoke the chain to get the hypothetical document.\n",
    "    return chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "979a89e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- HYPOTHETICAL DOCUMENT ---\n",
      "Steve Jobs was forced out of Apple in a boardroom coup on September 17, 1985.  The decision came after a series of internal conflicts and declining market share for Apple's products, particularly in the face of rising competition from IBM-compatible PCs. Jobs' visionary leadership and aggressive management style had proven increasingly incompatible with the company's direction.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's test our function.\n",
    "query = 'When was Steve Jobs fired from Apple?'\n",
    "hypothetical_doc = generate_hypothetical_document(query=query)\n",
    "print(\"--- HYPOTHETICAL DOCUMENT ---\")\n",
    "print(hypothetical_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67fcc5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- REAL DOCUMENTS RETRIEVED FOR: 'When was Steve Jobs fired from Apple?' ---\n",
      "- In 1985, Jobs departed Apple after a long power struggle with the company's board and its then-CEO, John Sculley. That same year, Jobs took some Apple employees with him to found NeXT, a computer platform development company that specialized in computers for higher-education and business markets,\n",
      "\n",
      "- In 1997, Jobs returned to Apple as CEO after the company's acquisition of NeXT. He was largely responsible for reviving Apple, which was on the verge of bankruptcy. He worked closely with British designer Jony Ive to develop a line of products and services that had larger cultural ramifications,\n",
      "\n",
      "- Steven Paul Jobs (February 24, 1955 â€“ October 5, 2011) was an American businessman, inventor, and investor best known for co-founding the technology company Apple Inc. Jobs was also the founder of NeXT and chairman and majority shareholder of Pixar. He was a pioneer of the personal computer\n",
      "\n",
      "- services that had larger cultural ramifications, beginning with the \"Think different\" advertising campaign, and leading to the iMac, iTunes, Mac OS X, Apple Store, iPod, iTunes Store, iPhone, App Store, and iPad. Jobs was also a board member at Gap Inc. from 1999 to 2002. In 2003, Jobs was\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now, use the hypothetical document to retrieve real documents.\n",
    "# We are NOT using the original query for retrieval.\n",
    "retrieved_docs = base_retriever.invoke(hypothetical_doc)\n",
    "\n",
    "print(f\"--- REAL DOCUMENTS RETRIEVED FOR: '{query}' ---\")\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"- {doc.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fab6e72",
   "metadata": {},
   "source": [
    "### 4. Using the LangChain `HypotheticalDocumentEmbedder`\n",
    "LangChain provides a convenient wrapper, `HypotheticalDocumentEmbedder`, that automates the \"generate-then-embed\" process. It acts like a special embedding function that you can plug directly into your vector store setup. When you ask it to embed a query, it internally performs the HyDE process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a417f3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.hyde.base import HypotheticalDocumentEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ee63ca2-fixed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HyDE Embedder created successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- CORRECTED: Setup the HyDE Embedder ---\n",
    "\n",
    "# 1. Define the LLMChain that will generate the hypothetical documents.\n",
    "# This uses a pre-defined prompt for web search-style questions.\n",
    "hyde_embeddings = HypotheticalDocumentEmbedder.from_llm(\n",
    "    llm=llm, \n",
    "    base_embeddings=embeddings, # The base model to embed the generated doc.\n",
    "    prompt_key=\"web_search\"    # A pre-built prompt for this task.\n",
    ")\n",
    "\n",
    "print(\"HyDE Embedder created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66c402d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Query: When was Steve Jobs fired from Apple?\n",
      "Embedding dimension: 384\n",
      "Sample of embedding vector: [np.float64(-0.02763230912387371), np.float64(0.05564332380890846), np.float64(0.08019711077213287), np.float64(-0.0018339100060984492), np.float64(0.05279001221060753)]\n"
     ]
    }
   ],
   "source": [
    "# Now, let's test the HyDE embedder directly.\n",
    "# This will first generate a hypothetical document for our query and then embed it.\n",
    "result = hyde_embeddings.embed_query(query)\n",
    "\n",
    "print(f\"Original Query: {query}\")\n",
    "print(f\"Embedding dimension: {len(result)}\")\n",
    "print(f\"Sample of embedding vector: {result[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a2a468-18e3-4610-ad44-783ca89369d7",
   "metadata": {},
   "source": [
    "#### Using HyDE for Retrieval\n",
    "Now we can create a new vector store that uses our `hyde_embeddings` object as its embedding function. This means that at search time, the query will automatically be passed through the HyDE process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b28c284e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- REAL DOCUMENTS RETRIEVED (LANGCHAIN HyDE) FOR: 'When was Steve Jobs fired from Apple?' ---\n",
      "- In 1985, Jobs departed Apple after a long power struggle with the company's board and its then-CEO, John Sculley. That same year, Jobs took some Apple employees with him to found NeXT, a computer platform development company that specialized in computers for higher-education and business markets,\n",
      "\n",
      "- In 1997, Jobs returned to Apple as CEO after the company's acquisition of NeXT. He was largely responsible for reviving Apple, which was on the verge of bankruptcy. He worked closely with British designer Jony Ive to develop a line of products and services that had larger cultural ramifications,\n",
      "\n",
      "- Steven Paul Jobs (February 24, 1955 â€“ October 5, 2011) was an American businessman, inventor, and investor best known for co-founding the technology company Apple Inc. Jobs was also the founder of NeXT and chairman and majority shareholder of Pixar. He was a pioneer of the personal computer\n",
      "\n",
      "- services that had larger cultural ramifications, beginning with the \"Think different\" advertising campaign, and leading to the iMac, iTunes, Mac OS X, Apple Store, iPod, iTunes Store, iPhone, App Store, and iPad. Jobs was also a board member at Gap Inc. from 1999 to 2002. In 2003, Jobs was\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a new FAISS vector store, this time passing our HyDE embedder.\n",
    "hyde_vectorstore = FAISS.from_documents(docs, hyde_embeddings)\n",
    "hyde_retriever = hyde_vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# When we invoke this retriever, it will automatically use the HyDE process on the query.\n",
    "hyde_retrieved_docs = hyde_retriever.invoke(query)\n",
    "\n",
    "print(f\"--- REAL DOCUMENTS RETRIEVED (LANGCHAIN HyDE) FOR: '{query}' ---\")\n",
    "for doc in hyde_retrieved_docs:\n",
    "    print(f\"- {doc.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f280a82b-5813-4c90-99c5-0370483ba645",
   "metadata": {},
   "source": [
    "### ðŸ”‘ Key Takeaways\n",
    "\n",
    "* **HyDE Bridges the Semantic Gap**: HyDE addresses the problem that a user's question and a factual answer can be semantically distant. It works by searching for a *hypothetical answer* in the vector space, which is more likely to be close to the *real answers*.\n",
    "* **Workflow**: The process is: **Query -> Generate Hypothetical Document -> Embed Hypothetical Document -> Retrieve Real Documents.**\n",
    "* **LangChain Integration**: LangChain's `HypotheticalDocumentEmbedder` automates this process. It acts as a special embedding function that wraps an LLM and a base embedding model.\n",
    "* **Improves Retrieval for Vague Queries**: HyDE is particularly effective for short, ambiguous, or poorly formulated questions because it enriches them with a full, context-rich hypothetical answer before the search.\n",
    "* **Trade-offs**: HyDE introduces an extra LLM call at the beginning of every query, which increases latency and cost. It also carries a small risk that a wildly inaccurate hypothetical document could lead to irrelevant results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ultimate RAG Bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
