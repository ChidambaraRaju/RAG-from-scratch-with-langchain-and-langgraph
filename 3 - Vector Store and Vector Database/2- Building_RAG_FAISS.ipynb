{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a987e0f2-b8d4-4b52-9c9e-5b23d9a1012a",
   "metadata": {},
   "source": [
    "### ðŸ“– Where We Are\n",
    "\n",
    "**In the last notebook**, we built our first complete, end-to-end RAG system using **ChromaDB**. We learned how a vector store is the core of the retrieval process and how to build different types of RAG chains (simple, conversational, and LCEL-based).\n",
    "\n",
    "**In this notebook**, we'll reinforce those concepts by building another RAG system, but this time with a different vector store: **FAISS (Facebook AI Similarity Search)**. The goal is to see how LangChain's abstractions make it easy to swap out components and to understand the unique characteristics of FAISS, which is renowned for its incredible speed and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd53861c",
   "metadata": {},
   "source": [
    "### 1. Building a RAG System with Langchain and FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedc476b",
   "metadata": {},
   "source": [
    "### What is FAISS?\n",
    "\n",
    "**FAISS** is an open-source library developed by Facebook AI for highly efficient similarity search and clustering of dense vectors. It's not a standalone database server like ChromaDB, but rather a powerful toolkit for building search indices.\n",
    "\n",
    "**Analogy: A High-Speed In-Memory Index âš¡**\n",
    "\n",
    "- If **ChromaDB** is like a self-contained, persistent local library on your computer, **FAISS** is like a high-speed, in-memory search index used by major search engines. \n",
    "- It's designed for pure, blazing-fast performance. You create the index in your application's memory and can then save it to disk to be reloaded later. This makes it incredibly fast for applications where the entire index can fit in RAM.\n",
    "\n",
    "**Key Advantages:**\n",
    "- **Extremely Fast**: Optimized for speed, capable of searching billions of vectors.\n",
    "- **Memory Efficient**: Uses advanced indexing techniques to reduce memory footprint.\n",
    "- **Scalable**: Can leverage GPU acceleration for even greater performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9138a0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Load Libraries ---\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LangChain core components for building chains and prompts.\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# LangChain specific components for the RAG pipeline.\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "# This is our new vector store for this notebook.\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Load environment variables from a .env file.\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "503f84d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 4 documents as chunks.\n"
     ]
    }
   ],
   "source": [
    "# --- 1 & 2. Document Loading and Splitting ---\n",
    "# We will manually create Document objects to focus on the FAISS-specific parts.\n",
    "sample_documents = [\n",
    "    Document(\n",
    "        page_content=\"Artificial Intelligence (AI) is the simulation of human intelligence in machines.\",\n",
    "        metadata={\"source\": \"AI Introduction\", \"topic\": \"AI\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Machine Learning is a subset of AI that enables systems to learn from data.\",\n",
    "        metadata={\"source\": \"ML Basics\", \"topic\": \"ML\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Deep Learning is a subset of machine learning based on artificial neural networks.\",\n",
    "        metadata={\"source\": \"Deep Learning\", \"topic\": \"DL\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Natural Language Processing (NLP) is a branch of AI that helps computers understand human language.\",\n",
    "        metadata={\"source\": \"NLP Overview\", \"topic\": \"NLP\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "# Since our documents are already small, we can skip the splitting step for this example.\n",
    "chunks = sample_documents\n",
    "print(f\"Using {len(chunks)} documents as chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d005d779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Embedding Generation ---\n",
    "# We'll use the same trusted Hugging Face model as before.\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1b1b36",
   "metadata": {},
   "source": [
    "### 4. Creating and Persisting a FAISS Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea637d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS vector store created in memory with 4 vectors.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Vector Storage ---\n",
    "# The `FAISS.from_documents` method is a convenient way to create an in-memory FAISS index.\n",
    "# It takes our document chunks, uses the embedding model to create vectors, and stores them in the index.\n",
    "vectorstore = FAISS.from_documents(documents=chunks, embedding=embeddings)\n",
    "print(f\"FAISS vector store created in memory with {vectorstore.index.ntotal} vectors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4d5f6a-b7c8-4d9e-0f1g-h2i3j4k5l6m7",
   "metadata": {},
   "source": [
    "Since FAISS is an in-memory index, we need to explicitly save it to disk if we want to use it later without re-creating it. This is a key difference from database-backed stores like ChromaDB which persist automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1243e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store saved to 'faiss_index' directory.\n"
     ]
    }
   ],
   "source": [
    "# The `save_local` method serializes the index and documents and saves them to a folder.\n",
    "vectorstore.save_local(folder_path=\"faiss_index\")\n",
    "print(\"Vector store saved to 'faiss_index' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56c01414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vector store with 4 vectors.\n"
     ]
    }
   ],
   "source": [
    "# To use the saved index, we load it back into memory.\n",
    "loaded_vectorstore = FAISS.load_local(\n",
    "    folder_path=\"faiss_index\",\n",
    "    embeddings=embeddings, \n",
    "    # This is required to load the index with pickled documents. Only use with trusted sources.\n",
    "    allow_dangerous_deserialization=True \n",
    ")\n",
    "print(f\"Loaded vector store with {loaded_vectorstore.index.ntotal} vectors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7g8h9i0j-k1l2-m3n4-o5p6-q7r8s9t0u1v2",
   "metadata": {},
   "source": [
    "### 5. Similarity Search with FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20cca69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is deep learning?\n",
      "\n",
      "Top 2 similar chunks:\n",
      "- Deep Learning is a subset of machine learning based on artificial neural networks. (Source: Deep Learning)\n",
      "- Machine Learning is a subset of AI that enables systems to learn from data. (Source: ML Basics)\n"
     ]
    }
   ],
   "source": [
    "query = \"What is deep learning?\"\n",
    "\n",
    "# The `similarity_search` method works just like it did with ChromaDB.\n",
    "results = loaded_vectorstore.similarity_search(query, k=2)\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Top 2 similar chunks:\")\n",
    "for doc in results:\n",
    "    print(f\"- {doc.page_content} (Source: {doc.metadata['source']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4895ab60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity search with scores:\n",
      "- Score: 0.5146, Content: Deep Learning is a subset of machine learning based on artificial neural networks.\n",
      "- Score: 0.9469, Content: Machine Learning is a subset of AI that enables systems to learn from data.\n"
     ]
    }
   ],
   "source": [
    "# `similarity_search_with_score` returns the L2 distance. A smaller score is better.\n",
    "results_with_scores = loaded_vectorstore.similarity_search_with_score(query=query, k=2)\n",
    "print(\"\\nSimilarity search with scores:\")\n",
    "for doc, score in results_with_scores:\n",
    "    print(f\"- Score: {score:.4f}, Content: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a100d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtered search results (topic=ML):\n",
      "Machine Learning is a subset of AI that enables systems to learn from data.\n"
     ]
    }
   ],
   "source": [
    "# FAISS also supports metadata filtering, a powerful feature for refining search results.\n",
    "filter_dict = {\"topic\": \"ML\"}\n",
    "filtered_results = loaded_vectorstore.similarity_search(\n",
    "    query=\"Tell me about learning from data\",\n",
    "    k=1,\n",
    "    filter=filter_dict\n",
    ")\n",
    "print(\"\\nFiltered search results (topic=ML):\")\n",
    "print(filtered_results[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989a459d",
   "metadata": {},
   "source": [
    "### 6. Building RAG Chains with FAISS and LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acd5606a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our LLM, in this case, a fast model from Groq.\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "llm = ChatGroq(model=\"gemma2-9b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "544c2266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our loaded FAISS vector store into a retriever.\n",
    "# The retriever is the standard LangChain interface for fetching documents.\n",
    "retriever = loaded_vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 2} # Fetch the top 2 documents\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8b7b25-db98-4c12-886d-1a0664871e98",
   "metadata": {},
   "source": [
    "#### Simple RAG Chain\n",
    "This is the most basic RAG chain. It takes a question, retrieves context, and generates an answer. It is stateless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "965bcda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Simple RAG Chain Test ---\n",
      "Q: What is the relationship between AI and Deep Learning?\n",
      "A: Deep Learning is a subset of Artificial Intelligence. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Define our prompt template using LCEL.\n",
    "simple_prompt = ChatPromptTemplate.from_template(\"\"\"Answer the question based only on the following context:\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\"\"\")\n",
    "\n",
    "# A helper function to format the retrieved documents.\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Build the RAG chain using the pipe operator.\n",
    "simple_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | simple_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"--- Simple RAG Chain Test ---\")\n",
    "question = \"What is the relationship between AI and Deep Learning?\"\n",
    "answer = simple_chain.invoke(question)\n",
    "print(f\"Q: {question}\")\n",
    "print(f\"A: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44d371b-a53d-4292-8097-f58c7344933a",
   "metadata": {},
   "source": [
    "#### Streaming RAG Chain\n",
    "LCEL makes streaming incredibly simple. By using the `.stream()` method instead of `.invoke()`, we can get the response back token by token as it's generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1317141e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Streaming RAG Chain Test ---\n",
      "Q: What is NLP?\n",
      "A: NLP is a branch of AI that helps computers understand human language.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The chain for streaming is identical to the simple chain.\n",
    "streaming_rag_chain = simple_chain\n",
    "\n",
    "print(\"--- Streaming RAG Chain Test ---\")\n",
    "question = \"What is NLP?\"\n",
    "print(f\"Q: {question}\")\n",
    "print(\"A: \", end=\"\", flush=True)\n",
    "# We use the .stream() method to get a token-by-token response.\n",
    "for chunk in streaming_rag_chain.stream(question):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a2a468-18e3-4610-ad44-783ca89369d7",
   "metadata": {},
   "source": [
    "#### Conversational RAG Chain\n",
    "This chain maintains a memory of the conversation, allowing it to understand follow-up questions that refer to previous turns in the dialogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a61576b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This prompt now includes a placeholder for chat history.\n",
    "conversational_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant. Use the provided context to answer questions.\"),\n",
    "    # `MessagesPlaceholder` is a special class that formats a list of messages.\n",
    "    (\"placeholder\", \"{chat_history}\"),\n",
    "    (\"human\", \"Context: {context}\\n\\nQuestion: {input}\"),\n",
    "])\n",
    "\n",
    "# This chain is slightly different: it uses `RunnablePassthrough.assign` to dynamically add context.\n",
    "# The context is retrieved based on the `input` (the user's question) only.\n",
    "conversational_rag_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        context=lambda x: format_docs(retriever.invoke(x[\"input\"])))\n",
    "    | conversational_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e40ffa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Conversational RAG Example ---\n",
      "Q1: What is machine learning?\n",
      "A1: Machine learning is a subset of artificial intelligence (AI) that allows systems to learn from data without being explicitly programmed. \n",
      "\n",
      "Q2: What is a subset of it?\n",
      "A2: Based on the context provided, **Deep Learning** is a subset of machine learning. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Conversational RAG Example ---\")\n",
    "# We manually manage the chat history as a list of messages.\n",
    "chat_history = []\n",
    "\n",
    "# First question\n",
    "q1 = \"What is machine learning?\"\n",
    "print(f\"Q1: {q1}\")\n",
    "a1 = conversational_rag_chain.invoke({\"input\": q1, \"chat_history\": chat_history})\n",
    "print(f\"A1: {a1}\")\n",
    "\n",
    "# Update the history with the first interaction.\n",
    "chat_history.extend([HumanMessage(content=q1), AIMessage(content=a1)])\n",
    "\n",
    "# Follow-up question\n",
    "q2 = \"What is a subset of it?\" # 'it' refers to machine learning\n",
    "print(f\"Q2: {q2}\")\n",
    "# The chain uses the chat_history to understand the context of the follow-up question.\n",
    "a2 = conversational_rag_chain.invoke({\"input\": q2, \"chat_history\": chat_history})\n",
    "print(f\"A2: {a2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f280a82b-5813-4c90-99c5-0370483ba645",
   "metadata": {},
   "source": [
    "### ðŸ”‘ Key Takeaways\n",
    "\n",
    "* **FAISS is for Speed**: FAISS is a high-performance library for vector search, making it an excellent choice for applications requiring low-latency, in-memory retrieval.\n",
    "* **In-Memory Index**: Unlike database servers, a FAISS index is an in-memory object that you must explicitly `.save_local()` and `.load_local()` to persist and reuse.\n",
    "* **LangChain's Abstractions Shine**: The `VectorStore` interface in LangChain makes it trivial to switch from ChromaDB to FAISS. The methods for creating the store (`.from_documents`), searching (`.similarity_search`), and creating a retriever (`.as_retriever()`) are consistent.\n",
    "* **LCEL is Universal**: The same LCEL chain structure we used with ChromaDB works perfectly with a FAISS retriever, demonstrating the power and flexibility of building various RAG chains, including simple, conversational, and streaming pipelines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ultimate RAG Bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
