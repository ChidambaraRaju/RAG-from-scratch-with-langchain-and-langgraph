{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a987e0f2-b8d4-4b52-9c9e-5b23d9a1012a",
   "metadata": {},
   "source": [
    "### üìñ Where We Are\n",
    "\n",
    "**In the previous sections**, we built all the foundational components for a RAG system:\n",
    "1.  **Data Ingestion & Parsing (Notebooks 1-6)**: We learned how to load and clean data from any source into `Document` objects.\n",
    "2.  **Vector Embeddings (Notebooks 7-8)**: We mastered the art of converting those `Document` objects into numerical vectors (embeddings) that capture their semantic meaning.\n",
    "\n",
    "**In this new section**, we'll learn how to store and search through those embeddings efficiently using **Vector Stores**. This notebook will walk you through building your **first complete, end-to-end RAG system** using ChromaDB, a popular open-source vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4692c27",
   "metadata": {},
   "source": [
    "### 1. Building a RAG System with LangChain and ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbba7810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from a .env file for secure key management.\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4237223f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the GROQ_API_KEY environment variable for authentication with the Groq service.\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ed9f09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Langchain Imports ---\n",
    "# Document Loaders for loading text and directory data.\n",
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "# Text Splitter for chunking documents.\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# HuggingFace Embeddings for creating vector representations of text.\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "# Chroma is our vector store for saving and retrieving embeddings.\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e1f2a3-b4c5-4d6e-8f9a-1c7b8d9e0f1g",
   "metadata": {},
   "source": [
    "### RAG (Retrieval-Augmented Generation) Architecture Overview\n",
    "\n",
    "The process of building a RAG system involves a series of sequential steps to transform raw data into a searchable knowledge base that an LLM can use.\n",
    "\n",
    "**Indexing Pipeline:**\n",
    "1.  **Document Loading**: Load documents from various sources (e.g., text files, PDFs, databases).\n",
    "2.  **Document Splitting**: Break large documents into smaller, semantically meaningful chunks.\n",
    "3.  **Embedding Generation**: Convert each chunk into a numerical vector using an embedding model.\n",
    "4.  **Vector Storage**: Store these embeddings in a specialized vector database like ChromaDB for efficient retrieval.\n",
    "\n",
    "**Retrieval and Generation Pipeline:**\n",
    "\n",
    "5.  **User Query**: A user asks a question.\n",
    "6.  **Query Embedding**: The user's query is also converted into a vector.\n",
    "7.  **Similarity Search**: The vector store searches for the document chunks with embeddings most similar to the query's embedding.\n",
    "8.  **Context Augmentation**: The retrieved chunks (the context) are combined with the original user query into a single prompt.\n",
    "9.  **Response Generation**: This augmented prompt is sent to an LLM, which generates an answer based on the provided context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71f63e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 documents.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Document Loading ---\n",
    "# Create sample documents and save them to a directory to simulate a real-world scenario.\n",
    "sample_docs = [\n",
    "    \"Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed.\",\n",
    "    \"Deep learning is a subset of machine learning based on artificial neural networks with many layers.\",\n",
    "    \"NLP is a field of AI that focuses on the interaction between computers and human language.\"\n",
    "]\n",
    "os.makedirs(\"text_files\", exist_ok=True)\n",
    "for i, doc in enumerate(sample_docs):\n",
    "    with open(f\"text_files/doc_{i}.txt\", \"w\") as f:\n",
    "        f.write(doc)\n",
    "\n",
    "# Use DirectoryLoader to load all the text files from our directory.\n",
    "loader = DirectoryLoader(\"text_files\", glob=\"*.txt\", loader_cls=TextLoader)\n",
    "documents = loader.load()\n",
    "print(f\"Loaded {len(documents)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc6cbdf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 3 chunks from 3 documents.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Document Splitting ---\n",
    "# Initialize a text splitter to break documents into smaller, manageable chunks.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  # The maximum size of each chunk.\n",
    "    chunk_overlap=50 # The number of characters to overlap between chunks.\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Created {len(chunks)} chunks from {len(documents)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a123de4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\Ultimate RAG Bootcamp\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Embedding Generation ---\n",
    "# Initialize the embedding model from HuggingFace. This runs locally.\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f516d25e-38e8-4679-b883-7313a968a356",
   "metadata": {},
   "source": [
    "### 4. Vector Storage with ChromaDB\n",
    "\n",
    "Now we arrive at the core of our retrieval system: the **Vector Store**. After creating embeddings for all our document chunks, we need a place to store them so we can search through them later.\n",
    "\n",
    "**Analogy: The GPS Navigation System üõ∞Ô∏è**\n",
    "-   **Embeddings** are the unique GPS coordinates for each document chunk.\n",
    "-   A **Vector Store** is the powerful navigation system (like Google Maps). When you provide your current location (the embedding of your query), the system doesn't need to check every coordinate on Earth. It uses highly efficient algorithms to instantly find the closest and most relevant points of interest (the most similar document chunks).\n",
    "\n",
    "**ChromaDB** is a popular open-source vector store that's easy to use and can run entirely on your local machine. The `Chroma.from_documents` function is a convenient helper that automates the process: it takes our chunks, uses the provided embedding model to create vectors for them, and stores them in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bd52948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created with 30 vectors.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Vector Storage ---\n",
    "# Create a Chroma vector store from the document chunks.\n",
    "# This single command handles embedding each chunk and storing it in the database.\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=chunks,              # The document chunks to be stored.\n",
    "    embedding=embedding_model,     # The model to use for creating embeddings.\n",
    "    persist_directory=\"chroma_db\"  # The directory to save the database on disk.\n",
    ")\n",
    "\n",
    "print(f\"Vector store created with {vector_store._collection.count()} vectors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359d503c",
   "metadata": {},
   "source": [
    "### 5. Testing the Retrieval (Similarity Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "780a0d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is deep learning?\n",
      "\n",
      "Top similar chunk:\n",
      "Deep learning is a subset of machine learning based on artificial neural networks with many layers.\n"
     ]
    }
   ],
   "source": [
    "# The core function of a vector store is performing a similarity search.\n",
    "query = \"What is deep learning?\"\n",
    "# The `similarity_search` method embeds the query and finds the most similar document chunks.\n",
    "similar_docs = vector_store.similarity_search(query)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"\\nTop similar chunk:\")\n",
    "print(similar_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00866c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Content: Deep learning is a subset of machine learning based on artificial neural networks with many layers.\n",
      "Score: 0.5072\n",
      "\n",
      "Content: Deep learning is a subset of machine learning based on artificial neural networks. \n",
      "    These networks are inspired by the human brain and consist of layers of interconnected \n",
      "    nodes. Deep learning has revolutionized fields like computer vision, natural language \n",
      "    processing, and speech recognition. Convolutional Neural Networks (CNNs) are particularly \n",
      "    effective for image processing, while Recurrent Neural Networks (RNNs) and Transformers\n",
      "Score: 0.5434\n",
      "\n",
      "Content: Deep learning is a subset of machine learning based on artificial neural networks. \n",
      "    These networks are inspired by the human brain and consist of layers of interconnected \n",
      "    nodes. Deep learning has revolutionized fields like computer vision, natural language \n",
      "    processing, and speech recognition. Convolutional Neural Networks (CNNs) are particularly \n",
      "    effective for image processing, while Recurrent Neural Networks (RNNs) and Transformers\n",
      "Score: 0.5434\n"
     ]
    }
   ],
   "source": [
    "# For more control, `similarity_search_with_score` returns the documents and their distance scores.\n",
    "# `k` determines the number of results to return.\n",
    "results_with_scores = vector_store.similarity_search_with_score(query, k=3)\n",
    "\n",
    "for doc, score in results_with_scores:\n",
    "    print(f\"\\nContent: {doc.page_content}\")\n",
    "    # For ChromaDB's default L2 distance, a LOWER score means MORE similar.\n",
    "    print(f\"Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22856c45",
   "metadata": {},
   "source": [
    "### 6. Building the RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a836d464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialize the LLM ---\n",
    "# We'll use Groq for its fast inference speed.\n",
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(\n",
    "    model=\"openai/gpt-oss-20b\", \n",
    "    temperature=0.2      # A low temperature for more factual, less creative responses.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e68d06-1ddf-44e2-9b26-a320c8c156c7",
   "metadata": {},
   "source": [
    "#### The Retriever\n",
    "A **Retriever** is a generic LangChain interface that fetches documents. A vector store can be easily converted into a retriever. This abstraction makes it easy to swap different retrieval methods or vector stores in your RAG chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aac2bb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the vector store into a retriever.\n",
    "# `search_kwargs={\"k\": 2}` specifies that the retriever should fetch the top 2 most similar documents.\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b5fd79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define the Prompt Template ---\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Keep the answer concise.\n",
    "\n",
    "Context: {context}\"\"\"\n",
    "\n",
    "# The prompt template includes placeholders for the context (from the retriever) and the user's input.\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a2a468-18e3-4610-ad44-783ca89369d7",
   "metadata": {},
   "source": [
    "#### Creating the RAG Chain\n",
    "Now we'll wire all the components together into a complete chain. We use two main helper functions:\n",
    "1.  **`create_stuff_documents_chain`**: This takes the retrieved documents and \"stuffs\" them all into the `{context}` placeholder in our prompt.\n",
    "2.  **`create_retrieval_chain`**: This is the final step. It combines the `retriever` (to get documents) and the `document_chain` (to process them with the LLM) into a single, runnable pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1db634c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# Create the chain that will process the retrieved documents.\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# Create the final retrieval chain.\n",
    "rag_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f694d72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep learning is a branch of machine learning that uses artificial neural networks with many layers (deep networks) to learn representations and patterns from data. Inspired by the human brain, these networks consist of interconnected nodes organized in layers, enabling breakthroughs in areas such as computer vision, natural language processing, and speech recognition.\n"
     ]
    }
   ],
   "source": [
    "# --- 7. Query the RAG system ---\n",
    "# The .invoke() method runs the entire pipeline.\n",
    "response = rag_chain.invoke({\"input\": \"What is Deep Learning?\"})\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bb7691",
   "metadata": {},
   "source": [
    "### Alternative: Building a RAG Chain with LCEL\n",
    "\n",
    "**LCEL (LangChain Expression Language)** is the modern, more powerful way to build chains. It uses the pipe operator `|` to connect components, offering greater flexibility and transparency.\n",
    "\n",
    "**Analogy: LEGOs vs. a Pre-built Toy üß±**\n",
    "-   **`create_retrieval_chain`** is like a pre-built toy car. It's easy to use and works great for its intended purpose.\n",
    "-   **LCEL** is like a box of LEGOs. You can build the same car, but you can also easily modify it, add new parts, or see exactly how each piece connects to the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ae14b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep learning is a subset of machine learning that uses artificial neural networks with many layers. These networks are inspired by the human brain and consist of layers of interconnected nodes. Deep learning has transformed fields such as computer vision, natural language processing, and speech recognition, with specialized architectures like Convolutional Neural Networks (CNNs) for image processing and Recurrent Neural Networks (RNNs) and Transformers for sequential data.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# A helper function to format the retrieved documents into a single string.\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Create a custom prompt\n",
    "custom_prompt = ChatPromptTemplate.from_template(\"\"\"Use the following context to answer the question. \n",
    "If you don't know the answer based on the context, say you don't know.\n",
    "Provide specific details from the context to support your answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\")\n",
    "\n",
    "# --- Build the chain using LCEL ---\n",
    "rag_chain_lcel = (\n",
    "    # This dictionary defines the inputs to the prompt.\n",
    "    # The retriever is called first, its output is piped to format_docs, and the result fills `context`.\n",
    "    # `question` is passed through directly from the input.\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_prompt          # The formatted inputs are piped into the prompt.\n",
    "    | llm             # The prompt is piped into the LLM.\n",
    "    | StrOutputParser() # The LLM's output is parsed into a simple string.\n",
    ")\n",
    "\n",
    "# Invoke the LCEL chain (note the simpler input format).\n",
    "response_lcel = rag_chain_lcel.invoke(\"What is Deep Learning?\")\n",
    "print(response_lcel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab450fb8",
   "metadata": {},
   "source": [
    "### 8. Adding New Documents to the Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97ead90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2386d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 3 new chunks. Total vectors now: 33\n"
     ]
    }
   ],
   "source": [
    "# Define a new document to add.\n",
    "\n",
    "new_document = \"\"\"\n",
    "Reinforcement Learning in Detail\n",
    "\n",
    "Reinforcement learning (RL) is a type of machine learning where an agent learns to make \n",
    "decisions by interacting with an environment. The agent receives rewards or penalties \n",
    "based on its actions and learns to maximize cumulative reward over time. Key concepts \n",
    "in RL include: states, actions, rewards, policies, and value functions. Popular RL \n",
    "algorithms include Q-learning, Deep Q-Networks (DQN), Policy Gradient methods, and \n",
    "Actor-Critic methods. RL has been successfully applied to game playing (like AlphaGo), \n",
    "robotics, and autonomous systems.\n",
    "\"\"\"\n",
    "# Add new documents to the existing vector store\n",
    "new_doc = Document(page_content=new_document, metadata={\"source\": \"Manual Entry\"})\n",
    "\n",
    "# Split the new document into chunks.\n",
    "new_chunks = text_splitter.split_documents([new_doc])\n",
    "\n",
    "# Add the new chunks to the existing vector store.\n",
    "vector_store.add_documents(new_chunks)\n",
    "\n",
    "print(f\"Added {len(new_chunks)} new chunks. Total vectors now: {vector_store._collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe09cabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinforcement learning (RL) is a type of machine learning in which an **agent** learns to make decisions by **interacting with an environment**. The agent receives **rewards or penalties** based on the actions it takes and learns to **maximize cumulative reward over time**. Key concepts in RL include:\n",
      "\n",
      "- **States** ‚Äì the current situation of the environment  \n",
      "- **Actions** ‚Äì the choices the agent can make  \n",
      "- **Rewards** ‚Äì feedback signals that indicate the desirability of an action  \n",
      "- **Policies** ‚Äì strategies that map states to actions  \n",
      "- **Value functions** ‚Äì estimates of future rewards for states or state‚Äëaction pairs  \n",
      "\n",
      "Popular RL algorithms mentioned are **Q‚Äëlearning**, **Deep Q‚ÄëNetworks (DQN)**, and **Policy Gradient methods**.\n"
     ]
    }
   ],
   "source": [
    "# Test the RAG system with a question about the new content.\n",
    "new_question = \"What is reinforcement learning?\"\n",
    "new_response = rag_chain_lcel.invoke(new_question)\n",
    "print(new_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1abc95d",
   "metadata": {},
   "source": [
    "### 9. Advanced RAG: Conversational Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538b14c9",
   "metadata": {},
   "source": [
    "A standard RAG chain is stateless; it has no memory of past interactions. This is a problem for follow-up questions (e.g., User: \"What is ML?\", Bot: ..., User: \"What are **its** main types?\"). The retriever won't know that \"its\" refers to machine learning.\n",
    "\n",
    "The solution is to create a **history-aware retriever**. This special retriever first looks at the chat history and the new question, and **reformulates** the new question into a standalone query. In the example above, it would transform \"What are its main types?\" into \"What are the main types of machine learning?\" before searching for documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10518cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c0d0f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Create a prompt for query reformulation ---\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \n",
    "which might reference context in the chat history, formulate a standalone question \n",
    "which can be understood without the chat history. Do NOT answer the question, \n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "\n",
    "# `MessagesPlaceholder` is a special variable that will hold the list of chat history messages.\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", contextualize_q_system_prompt),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc08ec23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Create the history-aware retriever ---\n",
    "# This chain will take the user input and chat history, and use the LLM to create a new, standalone query.\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44d74fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Create the final conversational RAG chain ---\n",
    "# First, we need a new QA prompt that also includes the chat history.\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Keep the answer concise.\n",
    "\n",
    "Context: {context}\"\"\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", qa_system_prompt),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "# This document chain will be used to generate the final answer.\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "# This is the final chain that connects the history-aware retriever and the QA chain.\n",
    "conversational_rag_chain = create_retrieval_chain(\n",
    "    history_aware_retriever, \n",
    "    question_answer_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "581ffeab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is machine learning?\n",
      "A: Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the chat history as an empty list.\n",
    "chat_history = []\n",
    "\n",
    "# First question\n",
    "first_question = \"What is machine learning?\"\n",
    "result1 = conversational_rag_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": first_question\n",
    "})\n",
    "print(f\"Q: {first_question}\")\n",
    "print(f\"A: {result1['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a796ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Update the chat history ---\n",
    "# We must manually update the history with the user's question and the model's answer.\n",
    "chat_history.extend([\n",
    "    HumanMessage(content=first_question),\n",
    "    AIMessage(content=result1['answer'])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1886bf2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: What is a subset of it?\n",
      "A: A subset of machine learning is one of its three main types: supervised learning, unsupervised learning, or reinforcement learning.\n"
     ]
    }
   ],
   "source": [
    "# Follow-up question\n",
    "follow_up_question = \"What is a subset of it?\"  # This question relies on the previous context.\n",
    "result2 = conversational_rag_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": follow_up_question\n",
    "})\n",
    "print(f\"\\nQ: {follow_up_question}\")\n",
    "print(f\"A: {result2['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f280a82b-5813-4c90-99c5-0370483ba645",
   "metadata": {},
   "source": [
    "### üîë Key Takeaways\n",
    "\n",
    "* **Vector Store is the Core**: A Vector Store (like ChromaDB) is a specialized database that stores embeddings and performs ultra-fast similarity searches, forming the heart of the retrieval system.\n",
    "* **End-to-End Pipeline**: A full RAG pipeline involves loading, splitting, embedding, and storing data (indexing), followed by retrieving relevant documents and generating an answer (retrieval & generation).\n",
    "* **LCEL for Flexibility**: LangChain Expression Language (LCEL) with the pipe `|` operator is the modern, powerful way to build custom, transparent RAG chains.\n",
    "* **Vector Stores are Dynamic**: You can easily add new documents to an existing vector store to keep your RAG system's knowledge base up-to-date.\n",
    "* **Memory is a Must for Conversation**: For building chatbots, a stateless RAG chain is not enough. A **history-aware retriever** is essential to reformulate follow-up questions, allowing the system to understand conversational context."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ultimate RAG Bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
