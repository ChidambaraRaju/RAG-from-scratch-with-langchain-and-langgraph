{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a987e0f2-b8d4-4b52-9c9e-5b23d9a1012a",
   "metadata": {},
   "source": [
    "### üìñ Where We Are\n",
    "\n",
    "**In the previous notebooks**, we explored two types of self-hosted vector stores:\n",
    "1.  **ChromaDB**: A developer-friendly, local database perfect for prototyping.\n",
    "2.  **FAISS**: A high-performance library for fast, in-memory similarity search.\n",
    "\n",
"**In this notebook**, we'll graduate to a **fully managed, cloud-native vector database: Pinecone**. This represents the type of infrastructure used in production-scale applications. We'll learn how to set up a Pinecone index, integrate it with LangChain, and see how the core RAG concepts remain the same even when using a powerful cloud service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd53861c",
   "metadata": {},
   "source": [
    "### 1. Building a RAG System with Langchain and Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedc476b",
   "metadata": {},
   "source": [
    "### What is Pinecone?\n",
    "\n",
    "**Pinecone** is a fully managed vector database designed for production environments. It provides a simple API to build high-performance, scalable AI applications that use vector search.\n",
    "\n",
    "**Analogy: A Cloud-Based Digital Library Service ‚òÅÔ∏è**\n",
    "\n",
    "- If **ChromaDB** is a local library and **FAISS** is a high-speed search index for that library, then **Pinecone** is a massive, cloud-based digital library service like Amazon's Kindle Store or Google Books.\n",
    "- You don't manage the servers, storage, or infrastructure. You simply upload your documents (vectors) via an API, and the service handles the complexity of scaling, security, and providing ultra-fast search results to millions of users.\n",
    "\n",
    "**Key Advantages:**\n",
    "- **Fully Managed**: No need to worry about servers or infrastructure.\n",
    "- **Highly Scalable**: Designed to handle billions of vectors with low latency.\n",
    "- **Production-Ready**: Offers features like real-time indexing, metadata filtering, and high availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MJmJwlQMPo4J",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is for securely storing your Pinecone API key.\n",
    "# Replace \"your pinecone api key\" with your actual key from app.pinecone.io\n",
    "api_key = \"your_pinecone_api_key\""
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Install the necessary libraries for LangChain and Pinecone.\n",
    "!pip install -qU langchain langchain-pinecone langchain-huggingface"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Import the Pinecone client library and initialize it with your API key.\n",
    "# This `pc` object is your main entry point for interacting with the Pinecone service.\n",
    "from pinecone import Pinecone\n",
    "pc = Pinecone(api_key=api_key)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# We'll use a standard open-source model from Hugging Face for our embeddings.\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3e4d5f6a-b7c8-4d9e-0f1g-h2i3j4k5l6m7",
   "metadata": {},
   "source": [
    "### 2. Creating a Pinecone Index\n",
    "\n",
    "In Pinecone, an **index** is a dedicated, isolated environment where your vectors are stored and searched. You need to create an index before you can add any data. The most critical step is ensuring the `dimension` of the index matches the output dimension of your embedding model."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from pinecone import ServerlessSpec\n",
    "\n",
    "# Define the name for your Pinecone index.\n",
    "index_name = \"rag-demo-index\"\n",
    "\n",
    "# Check if an index with this name already exists. This prevents errors on re-running the code.\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    print(f\"Creating index: {index_name}\")\n",
    "    # Create a new index.\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        # The dimension MUST match the embedding model's output dimension (384 for all-MiniLM-L6-v2).\n",
    "        dimension=384,\n",
    "        # We'll use cosine similarity, which is great for semantic search.\n",
    "        metric=\"cosine\",\n",
    "        # `ServerlessSpec` defines the cloud infrastructure; it's a cost-effective, auto-scaling option.\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "else:\n",
    "    print(f\"Index '{index_name}' already exists.\")\n",
    "\n",
    "# Connect to the specific index we'll be working with.\n",
    "index = pc.Index(index_name)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7g8h9i0j-k1l2-m3n4-o5p6-q7r8s9t0u1v2",
   "metadata": {},
   "source": [
    "### 3. Integrating with LangChain\n",
    "The `PineconeVectorStore` class is the LangChain wrapper that connects our Pinecone index to the standard `VectorStore` interface. This allows us to use it seamlessly in our RAG chains, just like we did with Chroma and FAISS."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Import the LangChain wrapper for Pinecone.\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# Create a LangChain VectorStore object that is connected to our Pinecone index.\n",
    "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Let's create some sample LangChain Document objects.\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"Building an exciting new project with LangChain - come check it out!\",\n",
    "        metadata={\"source\": \"tweet\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n",
    "        metadata={\"source\": \"news\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n",
    "        metadata={\"source\": \"tweet\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n",
    "        metadata={\"source\": \"news\"},\n",
    "    )\n",
    "]"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# The `.add_documents()` method handles the embedding and upserting process for us.\n",
    "# 'Upserting' means it will update a vector if an ID already exists, or insert it if it's new.\n",
    "vector_store.add_documents(documents=documents)\n",
    "print(\"Documents have been added to the Pinecone index.\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1d8b7b25-db98-4c12-886d-1a0664871e98",
   "metadata": {},
   "source": [
    "### 4. Querying the Vector Store\n",
    "Now that our vectors are stored in Pinecone, we can perform similarity searches. Managed databases like Pinecone have powerful features, including advanced metadata filtering, which allows you to narrow your search to only the documents that match certain criteria."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Perform a similarity search with a metadata filter.\n",
    "results = vector_store.similarity_search(\n",
    "    \"What is LangChain?\",  # The user's query.\n",
    "    k=2,                     # The number of results to return.\n",
    "    filter={\"source\": \"tweet\"} # Only search within documents where the source is 'tweet'.\n",
    ")\n",
    "\n",
    "print(\"Search results with metadata filter:\")\n",
    "for res in results:\n",
    "    print(f\"- {res.page_content} (Metadata: {res.metadata})\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e44d371b-a53d-4292-8097-f58c7344933a",
   "metadata": {},
   "source": [
    "### üìä Vector Store Comparison\n",
    "\n",
    "We've now seen three different types of vector stores, each with its own strengths.\n",
    "\n",
    "| Vector Store | Type | Persistence | Scalability | Best For |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **ChromaDB** | Local Database | Automatic (on disk) | Local Machine | Quick prototyping, local development, small to medium projects. |\n",
    "| **FAISS** | In-Memory Library | Manual (`save`/`load`) | Single Machine RAM | High-speed, low-latency search on a single machine where the index fits in memory. |\n",
    "| **Pinecone** | **Managed Cloud Service** | **Managed in Cloud** | **Cloud Scalable** | **Production applications, large-scale deployments, and when you don't want to manage infrastructure.** |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f280a82b-5813-4c90-99c5-0370483ba645",
   "metadata": {},
   "source": [
    "### üîë Key Takeaways\n",
    "\n",
    "* **Pinecone is a Managed Service**: Pinecone is a fully managed, cloud-native vector database designed for production-level scalability and performance. You interact with it via an API.\n",
    "* **Index is Key**: In Pinecone, you first create an **index**, which is a dedicated environment for your vectors. The `dimension` of this index **must** exactly match the output dimension of your embedding model.\n",
    "* **LangChain Integration is Seamless**: The `PineconeVectorStore` wrapper allows you to use your Pinecone index as a standard LangChain `VectorStore`, making it easy to integrate into RAG pipelines with methods like `.add_documents` and `.similarity_search`.\n",
    "* **Cloud-Native Benefits**: Managed databases like Pinecone excel at handling massive scale and provide powerful features like advanced metadata filtering without requiring you to manage any of the underlying infrastructure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGUdemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}