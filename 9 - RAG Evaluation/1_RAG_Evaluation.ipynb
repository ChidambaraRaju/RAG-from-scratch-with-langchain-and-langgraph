{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a987e0f2-b8d4-4b52-9c9e-5b23d9a1012a",
   "metadata": {},
   "source": [
    "### 📖 Where We Are\n",
    "\n",
    "**In the previous sections**, we have mastered building a variety of sophisticated RAG pipelines, from simple retrievers to complex, self-correcting agents. We've learned how to load data, embed it, and use different retrieval strategies to provide context to an LLM.\n",
    "\n",
    "**But how do we know if our RAG system is actually *good*?** How can we measure its performance, compare different versions, and identify areas for improvement? \n",
    "\n",
    "**In this new section on RAG Evaluation**, we will answer those questions. This notebook introduces the critical practice of evaluating RAG pipelines. We will learn how to use **LangSmith** to create test datasets and implement a powerful pattern called **LLM-as-a-Judge** to automatically score our system's performance on key metrics like correctness, relevance, and groundedness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b6ae34",
   "metadata": {},
   "source": [
    "### 1. The System Under Test: A Basic RAG Pipeline\n",
    "First, let's build the RAG application that we want to evaluate. This will be a standard pipeline that retrieves documents from a vector store and uses them to generate an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38d7ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Environment and Library Setup ---\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1b2c3d4-e5f6-a7b8-c9d0-e1f2a3b4c5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "c:\\Users\\USER\\Desktop\\Ultimate RAG Bootcamp\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## RAG\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# List of URLs to load documents from\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "# Load documents from the URLs\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "# Initialize a text splitter with specified chunk size and overlap\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "\n",
    "# Split the documents into chunks\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add the document chunks to the \"vector store\" using OpenAIEmbeddings\n",
    "vectorstore = InMemoryVectorStore.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=HuggingFaceEmbeddings(model=\"all-MiniLM-L6-v2\"),\n",
    ")\n",
    "\n",
    "# With langchain we can easily turn any vector store into a retrieval component:\n",
    "retriever = vectorstore.as_retriever(k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da8cd386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- The RAG Bot Function ---\n",
    "from langsmith import traceable\n",
    "\n",
    "# The `@traceable` decorator automatically logs the inputs, outputs, and any errors of this function to LangSmith.\n",
    "@traceable()\n",
    "def rag_bot(question: str) -> dict:\n",
    "    \"\"\"Our simple RAG pipeline function that we want to evaluate.\"\"\"\n",
    "    # 1. Retrieve relevant documents.\n",
    "    docs = retriever.invoke(question)\n",
    "    docs_string = \" \".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # 2. Create a system prompt with the retrieved context.\n",
    "    instructions = f\"\"\"You are a helpful assistant. Use the following source documents to answer the user's questions. \n",
    "    If you don't know the answer, just say that you don't know. Keep the answer concise.\n",
    "    Documents: {docs_string}\"\"\"\n",
    "    \n",
    "    # 3. Generate the final answer.\n",
    "    llm = ChatGroq(model=\"openai/gpt-oss-20b\")\n",
    "    ai_msg = llm.invoke([\n",
    "         {\"role\": \"system\", \"content\": instructions},\n",
    "         {\"role\": \"user\", \"content\": question},\n",
    "    ])\n",
    "    \n",
    "    # The function must return a dictionary with the 'answer' and the retrieved 'documents'.\n",
    "    \n",
    "    \n",
    "    return {\"answer\": ai_msg.content, \"documents\": docs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a2a468-18e3-4610-ad44-783ca89369d7",
   "metadata": {},
   "source": [
    "### 2. Creating an Evaluation Dataset in LangSmith\n",
    "\n",
    "To evaluate our system, we need a benchmark. In LangSmith, this is a **dataset** of questions and their corresponding ground-truth answers. This allows us to test our RAG bot's performance on a consistent set of examples.\n",
    "\n",
    "**Analogy: A Final Exam for Your RAG Bot 📝**\n",
    "\n",
    "Think of this dataset as the final exam for your RAG application. Each question-answer pair is like a question on the test. By running your bot against this exam, you can get a clear score on how well it performs and identify which questions it struggles with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9760d4f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "LangSmithConflictError",
     "evalue": "Conflict for /datasets. HTTPError('409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets', '{\"detail\":\"Dataset with this name already exists.\"}')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\Ultimate RAG Bootcamp\\.venv\\Lib\\site-packages\\langsmith\\utils.py:154\u001b[39m, in \u001b[36mraise_for_status_with_text\u001b[39m\u001b[34m(response)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\Ultimate RAG Bootcamp\\.venv\\Lib\\site-packages\\requests\\models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\Ultimate RAG Bootcamp\\.venv\\Lib\\site-packages\\langsmith\\client.py:837\u001b[39m, in \u001b[36mClient.request_with_retries\u001b[39m\u001b[34m(self, method, pathname, request_kwargs, stop_after_attempt, retry_on, to_ignore, handle_response, _context, **kwargs)\u001b[39m\n\u001b[32m    831\u001b[39m     response = \u001b[38;5;28mself\u001b[39m.session.request(\n\u001b[32m    832\u001b[39m         method,\n\u001b[32m    833\u001b[39m         _construct_url(\u001b[38;5;28mself\u001b[39m.api_url, pathname),\n\u001b[32m    834\u001b[39m         stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    835\u001b[39m         **request_kwargs,\n\u001b[32m    836\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m837\u001b[39m \u001b[43mls_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status_with_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\Ultimate RAG Bootcamp\\.venv\\Lib\\site-packages\\langsmith\\utils.py:156\u001b[39m, in \u001b[36mraise_for_status_with_text\u001b[39m\u001b[34m(response)\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m requests.HTTPError(\u001b[38;5;28mstr\u001b[39m(e), response.text) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mHTTPError\u001b[39m: [Errno 409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets] {\"detail\":\"Dataset with this name already exists.\"}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mLangSmithConflictError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Create the dataset in LangSmith.\u001b[39;00m\n\u001b[32m     23\u001b[39m dataset_name = \u001b[33m\"\u001b[39m\u001b[33mRAG_Evaluation_Test_v1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m dataset = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTest dataset for RAG pipeline evaluation.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Upload the examples to the dataset.\u001b[39;00m\n\u001b[32m     26\u001b[39m client.create_examples(dataset_id=dataset.id, examples=examples)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\Ultimate RAG Bootcamp\\.venv\\Lib\\site-packages\\langsmith\\client.py:3508\u001b[39m, in \u001b[36mClient.create_dataset\u001b[39m\u001b[34m(self, dataset_name, description, data_type, inputs_schema, outputs_schema, transformations, metadata)\u001b[39m\n\u001b[32m   3505\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m outputs_schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3506\u001b[39m     dataset[\u001b[33m\"\u001b[39m\u001b[33moutputs_schema_definition\u001b[39m\u001b[33m\"\u001b[39m] = outputs_schema\n\u001b[32m-> \u001b[39m\u001b[32m3508\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest_with_retries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3509\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3510\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/datasets\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3511\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mContent-Type\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mapplication/json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3512\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_orjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3513\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3514\u001b[39m ls_utils.raise_for_status_with_text(response)\n\u001b[32m   3516\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ls_schemas.Dataset(\n\u001b[32m   3517\u001b[39m     **response.json(),\n\u001b[32m   3518\u001b[39m     _host_url=\u001b[38;5;28mself\u001b[39m._host_url,\n\u001b[32m   3519\u001b[39m     _tenant_id=\u001b[38;5;28mself\u001b[39m._get_optional_tenant_id(),\n\u001b[32m   3520\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Desktop\\Ultimate RAG Bootcamp\\.venv\\Lib\\site-packages\\langsmith\\client.py:882\u001b[39m, in \u001b[36mClient.request_with_retries\u001b[39m\u001b[34m(self, method, pathname, request_kwargs, stop_after_attempt, retry_on, to_ignore, handle_response, _context, **kwargs)\u001b[39m\n\u001b[32m    877\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ls_utils.LangSmithNotFoundError(\n\u001b[32m    878\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResource not found for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpathname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    879\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_context\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    880\u001b[39m     )\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m response.status_code == \u001b[32m409\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ls_utils.LangSmithConflictError(\n\u001b[32m    883\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConflict for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpathname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m_context\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    884\u001b[39m     )\n\u001b[32m    885\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    886\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ls_utils.LangSmithError(\n\u001b[32m    887\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpathname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in LangSmith\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    888\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m API. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    889\u001b[39m     )\n",
      "\u001b[31mLangSmithConflictError\u001b[39m: Conflict for /datasets. HTTPError('409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets', '{\"detail\":\"Dataset with this name already exists.\"}')"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "# Initialize the LangSmith client.\n",
    "client = Client()\n",
    "\n",
    "# Define the questions and ground-truth answers for our test dataset.\n",
    "examples = [\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"How does the ReAct agent use self-reflection?\"},\n",
    "        \"outputs\": {\"answer\": \"ReAct integrates reasoning and acting by performing actions with tools and then observing the outputs to inform its next step.\"},\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What are the types of biases that can arise with few-shot prompting?\"},\n",
    "        \"outputs\": {\"answer\": \"The biases that can arise with few-shot prompting include majority label bias, recency bias, and common token bias.\"},\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What are five types of adversarial attacks on LLMs?\"},\n",
    "        \"outputs\": {\"answer\": \"Five types of adversarial attacks are token manipulation, gradient-based attacks, jailbreak prompting, human red-teaming, and model red-teaming.\"},\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create the dataset in LangSmith.\n",
    "dataset_name = \"RAG_Evaluation_Test_v1\"\n",
    "dataset = client.create_dataset(dataset_name=dataset_name, description=\"Test dataset for RAG pipeline evaluation.\")\n",
    "# Upload the examples to the dataset.\n",
    "client.create_examples(dataset_id=dataset.id, examples=examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69cd636",
   "metadata": {},
   "source": [
    "### 3. Implementing Evaluators (LLM-as-a-Judge)\n",
    "\n",
    "Manually grading hundreds of RAG outputs is impractical. Instead, we use a powerful pattern called **LLM-as-a-Judge**. We use another, powerful LLM (the \"judge\") and give it a very specific prompt (the \"rubric\") to automatically score our RAG system's performance on different metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e7c5ab-66a8-4c12-9e3f-8c3a51f0c2a1",
   "metadata": {},
   "source": [
    "#### Evaluator 1: Correctness\n",
    "This measures how factually similar the RAG system's answer is to the ground-truth answer in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c77670d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Annotated, TypedDict\n",
    "from langchain_groq import ChatGroq\n",
    "# Define the structured output schema for our grader.\n",
    "class CorrectnessGrade(TypedDict):\n",
    "    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]\n",
    "    correct: Annotated[bool, ..., \"True if the answer is correct, False otherwise.\"]\n",
    "\n",
    "# Define the detailed instructions (the \"rubric\") for the grader LLM.\n",
    "correctness_instructions = \"\"\"You are a teacher grading a quiz. You will be given a QUESTION, the GROUND TRUTH answer, and a STUDENT ANSWER. \n",
    "Grade the student's answer based ONLY on its factual accuracy compared to the ground truth. It is OK if the student's answer contains more information, as long as it does not contradict the ground truth.\"\"\"\n",
    "\n",
    "# Create the grader LLM, forcing it to return the structured output.\n",
    "grader_llm = ChatGroq(model=\"openai/gpt-oss-20b\", temperature=0).with_structured_output(CorrectnessGrade)\n",
    "\n",
    "# Define the evaluator function that LangSmith will call.\n",
    "def correctness(run, example) -> dict:\n",
    "    \"\"\"An evaluator for RAG answer accuracy.\"\"\"\n",
    "    inputs = example.inputs\n",
    "    outputs = run.outputs\n",
    "    reference_outputs = example.outputs\n",
    "    \n",
    "    # Combine the information into a single string for the grader.\n",
    "    answers = f\"\"\"QUESTION: {inputs['question']}\n",
    "    GROUND TRUTH ANSWER: {reference_outputs['answer']}\n",
    "    STUDENT ANSWER: {outputs['answer']}\"\"\"\n",
    "    \n",
    "    # Run the grader.\n",
    "    grade = grader_llm.invoke([{\"role\": \"system\", \"content\": correctness_instructions}, {\"role\": \"user\", \"content\": answers}])\n",
    "    return {\"key\": \"correctness\", \"score\": grade[\"correct\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4609f750",
   "metadata": {},
   "source": [
    "#### Evaluator 2: Groundedness\n",
    "This is one of the most important RAG metrics. It checks if the RAG system's answer is supported by the documents it retrieved. This is how we measure **hallucinations**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8c19adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroundedGrade(TypedDict):\n",
    "    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]\n",
    "    grounded: Annotated[bool, ..., \"Provide the score on if the answer is grounded in the facts.\"]\n",
    "\n",
    "grounded_instructions = \"\"\"You are a grader checking for hallucinations. You will be given a set of FACTS (retrieved documents) and a STUDENT ANSWER.\n",
    "Your task is to determine if the student's answer is fully supported by the provided facts. The answer is grounded if all claims made in it can be verified from the facts.\"\"\"\n",
    "\n",
    "grounded_llm = ChatGroq(model=\"openai/gpt-oss-20b\", temperature=0).with_structured_output(GroundedGrade)\n",
    "\n",
    "def groundedness(run, example) -> dict:\n",
    "    outputs = run.outputs\n",
    "    # The 'documents' must be returned by our RAG bot function to be used here.\n",
    "    doc_string = \"\\n\\n\".join(doc.page_content for doc in outputs[\"documents\"])\n",
    "    answer = f\"FACTS: {doc_string}\\nSTUDENT ANSWER: {outputs['answer']}\"\n",
    "    grade = grounded_llm.invoke([{\"role\": \"system\", \"content\": grounded_instructions}, {\"role\": \"user\", \"content\": answer}])\n",
    "    return {\"key\": \"groundedness\", \"score\": grade[\"grounded\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecd2691",
   "metadata": {},
   "source": [
    "### 4. Running the Evaluation\n",
    "Now we use the `evaluate` function to orchestrate the entire test. It will run our `rag_bot` on every example in our dataset and then apply each of our evaluator functions to the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8592c529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-evaluation-run-f18d3202' at:\n",
      "https://smith.langchain.com/o/5cf2e409-b5ba-4812-a6b3-85a090392b44/datasets/df74f216-0950-4396-b730-6f2ab1da6aee/compare?selectedSessions=beb68290-1d15-4131-b9bb-0376dd97ae31\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:16,  5.34s/it]\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "# Define the function to be evaluated.\n",
    "def target(inputs: dict) -> dict:\n",
    "    return rag_bot(inputs[\"question\"])\n",
    "\n",
    "# Run the evaluation.\n",
    "experiment_results = evaluate(\n",
    "    target, # The RAG system to test.\n",
    "    data=dataset_name, # The name of the LangSmith dataset.\n",
    "    evaluators=[correctness, groundedness], # The list of our custom grading functions.\n",
    "    experiment_prefix=\"rag-evaluation-run\", # A name for this test run.\n",
    "    metadata={\"version\": \"Initial RAG pipeline\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f280a82b-5813-4c90-9c5-0370483ba645",
   "metadata": {},
   "source": [
    "### 🔑 Key Takeaways\n",
    "\n",
    "* **Evaluation is Essential**: To build reliable RAG systems, you must move beyond anecdotal testing and adopt a structured evaluation process. This is the key to measuring performance and making targeted improvements.\n",
    "* **LangSmith for Experimentation**: LangSmith is a powerful platform for LLM Ops. It allows you to create curated **datasets** of test cases and run repeatable **experiments** to benchmark your application's performance.\n",
    "* **LLM-as-a-Judge**: This is a powerful and scalable pattern for automating evaluation. By using a strong LLM with a detailed prompt (a rubric) and a structured output schema, you can automatically grade your RAG system's outputs.\n",
    "* **Key Evaluation Metrics**: For a RAG system, you need to measure multiple aspects:\n",
    "    - **Correctness**: Is the answer factually accurate compared to a ground truth?\n",
    "    - **Groundedness**: Is the answer supported by the retrieved context? (Anti-hallucination metric)\n",
    "    - **Relevance**: Does the answer actually address the user's question?\n",
    "    - **Retrieval Relevance**: Did the retriever fetch useful documents in the first place?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ultimate RAG Bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
