{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a987e0f2-b8d4-4b52-9c9e-5b23d9a1012a",
   "metadata": {},
   "source": [
    "### ðŸ“– Where We Are\n",
    "\n",
    "**In the previous sections**, we mastered the end-to-end RAG pipeline using different vector stores (ChromaDB, FAISS, Pinecone). All of those systems relied on a single method for retrieval: **dense (semantic) search**, which finds documents based on their conceptual meaning.\n",
    "\n",
    "**In this new section on Hybrid Search Strategies**, we will learn how to make our retrieval process even more powerful. This notebook introduces **Hybrid Search**, a technique that combines the strengths of semantic search with traditional keyword search to create a more robust and accurate retrieval system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739343aa",
   "metadata": {},
   "source": [
    "### 1. Hybrid Search - Combining Dense and Sparse Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04fe3498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LangChain Imports ---\n",
    "# FAISS for our dense vector store.\n",
    "from langchain_community.vectorstores import FAISS\n",
    "# HuggingFace embeddings for our dense retriever.\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "# BM25Retriever for our sparse, keyword-based retriever.\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "# EnsembleRetriever to combine the results of multiple retrievers.\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "# Standard Document object.\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a9c6264",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\Ultimate RAG Bootcamp\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Create Sample Documents ---\n",
    "docs = [\n",
    "    Document(page_content=\"LangChain helps build LLM applications.\"),\n",
    "    Document(page_content=\"Pinecone is a vector database for semantic search.\"),\n",
    "    Document(page_content=\"The Eiffel Tower is located in Paris.\"),\n",
    "    Document(page_content=\"Langchain can be used to develop agentic ai application.\"),\n",
    "    Document(page_content=\"Langchain has many types of retrievers.\")\n",
    "]\n",
    "\n",
    "# --- Step 2: Set up the Dense Retriever ---\n",
    "# This is the semantic search component.\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "dense_vectorstore = FAISS.from_documents(docs, embedding_model)\n",
    "dense_retriever = dense_vectorstore.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a2a468-18e3-4610-ad44-783ca89369d7",
   "metadata": {},
   "source": [
    "#### The Sparse Retriever: BM25\n",
    "\n",
    "For our sparse retriever, we'll use **BM25 (Best Matching 25)**. This is a modern, industry-standard algorithm for keyword-based search. It's an evolution of TF-IDF and is highly effective at ranking documents based on the terms they share with a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "406be3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Set up the Sparse Retriever ---\n",
    "# This is the keyword search component.\n",
    "sparse_retriever = BM25Retriever.from_documents(docs)\n",
    "sparse_retriever.k = 3  # Set to retrieve the top 3 matching documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f99849-5e72-463d-82c8-7c858b1ab6b9",
   "metadata": {},
   "source": [
    "#### Combining Retrievers with `EnsembleRetriever`\n",
    "\n",
    "The `EnsembleRetriever` is a powerful LangChain component that takes a list of different retrievers and combines their results using a weighted scoring method (Reciprocal Rank Fusion). This is how we implement the hybrid search formula, with the `weights` parameter acting as our **Î±** value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf361d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Create the Hybrid Retriever ---\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    # A list of the retrievers we want to combine.\n",
    "    retrievers=[dense_retriever, sparse_retriever],\n",
    "    # The weights determine the balance between the retrievers.\n",
    "    # Here, we're giving more importance to the semantic (dense) search.\n",
    "    # The weights must sum to 1.0.\n",
    "    weights=[0.7, 0.3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d783d073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How can I build an application using LLMs?\n",
      "ðŸ”¹ Document 1: LangChain helps build LLM applications.\n",
      "ðŸ”¹ Document 2: Langchain can be used to develop agentic ai application.\n",
      "ðŸ”¹ Document 3: Pinecone is a vector database for semantic search.\n",
      "ðŸ”¹ Document 4: Langchain has many types of retrievers.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 5: Query and Get Results ---\n",
    "query = \"How can I build an application using LLMs?\"\n",
    "# When we invoke the hybrid retriever, it runs both dense and sparse searches,\n",
    "# combines the results, and returns a single, re-ranked list.\n",
    "results = hybrid_retriever.invoke(query)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"ðŸ”¹ Document {i+1}: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f12c1f6",
   "metadata": {},
   "source": [
    "### 2. RAG Pipeline with the Hybrid Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8b7b25-db98-4c12-886d-1a0664871e98",
   "metadata": {},
   "source": [
    "The beauty of LangChain's modular design is that we can now drop our new `hybrid_retriever` into the same RAG chain structure we've used before with no other changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8af0252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "# Load API keys.\n",
    "load_dotenv()\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53479ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM.\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.2)\n",
    "\n",
    "# Create the prompt template.\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Keep the answer concise.\n",
    "    Context: {context}\n",
    "    Question: {input}\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4377ed8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the two main components of the RAG chain.\n",
    "document_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
    "rag_chain = create_retrieval_chain(\n",
    "    retriever=hybrid_retriever, # Plug in our new hybrid retriever here!\n",
    "    combine_docs_chain=document_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d783d073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Answer: To build an app using LLMs (Large Language Models), you can leverage LangChain, a powerful framework that helps you develop agentic AI applications. Here's a step-by-step guide to get you started:\n",
      "\n",
      "1. **Choose a LangChain Retriever**: LangChain offers various types of retrievers, such as:\n",
      "\t* Embedding retriever: uses vector embeddings to retrieve relevant information.\n",
      "\t* SQL retriever: retrieves data from a SQL database.\n",
      "\t* HTTP retriever: retrieves data from an HTTP endpoint.\n",
      "\t* File retriever: retrieves data from a file.\n",
      "\t* Pinecone retriever: uses Pinecone, a vector database for semantic search.\n",
      "2. **Select a LangChain Agent**: LangChain agents are the core components that interact with the retriever and generate responses. You can choose from:\n",
      "\t* Chain agent: a basic agent that chains multiple actions together.\n",
      "\t* LLM agent: uses a pre-trained LLM to generate responses.\n",
      "\t* Hybrid agent: combines multiple agents to create a more complex response.\n",
      "3. **Design your App's Workflow**: Determine how your app will interact with the user and the retriever. This might involve:\n",
      "\t* User input: collect user input and pass it to the agent.\n",
      "\t* Agent processing: the agent processes the user input and retrieves relevant information.\n",
      "\t* Response generation: the agent generates a response based on the retrieved information.\n",
      "4. **Implement your App**: Use LangChain's SDK to implement your app's workflow. You can write code in Python or other supported languages.\n",
      "5. **Integrate with Pinecone (optional)**: If you choose to use Pinecone as your retriever, you'll need to integrate it with LangChain. This involves setting up a Pinecone index and configuring the LangChain retriever to use it.\n",
      "6. **Test and Deploy**: Test your app thoroughly and deploy it to a production environment.\n",
      "\n",
      "Here's some sample Python code to get you started:\n",
      "```python\n",
      "import langchain\n",
      "from langchain.agents import LLMAgent\n",
      "from langchain.chains import Chain\n",
      "\n",
      "# Create a Pinecone retriever\n",
      "retriever = langchain.PineconeRetriever(index_name=\"my_index\")\n",
      "\n",
      "# Create an LLM agent\n",
      "agent = LLMAgent(model_name=\"large-language-model\")\n",
      "\n",
      "# Create a chain agent\n",
      "chain = Chain(retriever=retriever, agent=agent)\n",
      "\n",
      "# Define a function to handle user input\n",
      "def handle_input(user_input):\n",
      "    response = chain.run(user_input)\n",
      "    return response\n",
      "\n",
      "# Test the app\n",
      "user_input = \"What is the capital of France?\"\n",
      "response = handle_input(user_input)\n",
      "print(response)\n",
      "```\n",
      "This code creates a Pinecone retriever, an LLM agent, and a chain agent. It then defines a function to handle user input and test the app with a sample question.\n",
      "\n",
      "Remember to replace the sample code with your own implementation and adjust the configuration to suit your app's specific needs.\n",
      "\n",
      "ðŸ“„ Source Documents:\n",
      "Doc 1: LangChain helps build LLM applications.\n",
      "Doc 2: Langchain can be used to develop agentic ai application.\n",
      "Doc 3: Pinecone is a vector database for semantic search.\n",
      "Doc 4: Langchain has many types of retrievers.\n"
     ]
    }
   ],
   "source": [
    "# Ask a question using the full RAG pipeline.\n",
    "query = {\"input\": \"How can I build an app using LLMs?\"}\n",
    "response = rag_chain.invoke(query)\n",
    "\n",
    "# Print the results.\n",
    "print(\"âœ… Answer:\", response[\"answer\"])\n",
    "print(\"\\nðŸ“„ Source Documents:\")\n",
    "for i, doc in enumerate(response[\"context\"]):\n",
    "    print(f\"Doc {i+1}: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f280a82b-5813-4c90-99c5-0370483ba645",
   "metadata": {},
   "source": [
    "### ðŸ”‘ Key Takeaways\n",
    "\n",
    "* **Hybrid Search is Powerful**: It combines dense (semantic) and sparse (keyword) search to overcome their individual weaknesses, leading to more robust and relevant retrieval.\n",
    "* **Dense vs. Sparse**: Dense retrieval understands meaning and context, while sparse retrieval excels at finding exact keywords, acronyms, and specific terms.\n",
    "* **`BM25Retriever` for Keywords**: This is LangChain's standard and highly effective component for sparse, keyword-based search.\n",
    "* **`EnsembleRetriever` for Combination**: This specialized retriever is the key to implementing hybrid search in LangChain. It combines results from multiple retrievers using a weighted scoring system.\n",
    "* **Modular and Flexible**: A hybrid retriever can be used as a drop-in replacement for any other retriever in a LangChain RAG pipeline, demonstrating the framework's flexibility."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ultimate RAG Bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
