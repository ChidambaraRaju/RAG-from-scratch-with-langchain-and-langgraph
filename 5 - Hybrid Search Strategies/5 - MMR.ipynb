{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a987e0f2-b8d4-4b52-9c9e-5b23d9a1012a",
   "metadata": {},
   "source": [
    "### ðŸ“– Where We Are\n",
    "\n",
    "**In the previous notebooks**, we explored powerful retrieval strategies like **Hybrid Search** (combining semantic and keyword search) and **Reranking** (using an LLM to refine results). These techniques help improve the *relevance* and *precision* of our retrieved documents.\n",
    "\n",
    "**In this notebook**, we'll tackle another common challenge in retrieval: **redundancy**. We will learn how to use **Maximal Marginal Relevance (MMR)**, a sophisticated retrieval method designed to increase the **diversity** of the documents we retrieve, providing the LLM with a broader and more comprehensive context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcf6a13",
   "metadata": {},
   "source": [
    "### 1. Maximal Marginal Relevance (MMR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b320d771-48d6-4448-9c4c-31d77a83d973",
   "metadata": {},
   "source": [
    "**MMR (Maximal Marginal Relevance)** is a powerful diversity-aware retrieval technique. It's designed to solve the problem where a standard similarity search returns multiple chunks that are all very similar to each other, offering little new information.\n",
    "\n",
    "MMR works by selecting documents that are both:\n",
    "1.  **Relevant** to the user's query.\n",
    "2.  **Diverse** (dissimilar) from the documents that have already been selected.\n",
    "\n",
    "This ensures the final context passed to the LLM is comprehensive and covers different aspects of the topic, rather than repeating the same point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87294dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LangChain Imports ---\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0340285d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Environment Setup ---\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cb5be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Load and Chunk Documents ---\n",
    "# This dataset contains several related but distinct points about LangChain.\n",
    "loader = TextLoader(\"langchain_rag_dataset.txt\")\n",
    "raw_docs = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(raw_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b95ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Create Vector Store ---\n",
    "# We'll use a FAISS vector store for its speed in this example.\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(chunks, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8b7b25-db98-4c12-886d-1a0664871e98",
   "metadata": {},
   "source": [
    "### 3. Creating a Standard vs. MMR Retriever\n",
    "To see the effect of MMR, we will create two retrievers: a standard similarity retriever and an MMR retriever. This will allow us to directly compare their outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c0699d-a1b2-c3d4-e5f6-a7b8c9d0e1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard Similarity Retriever ---\n",
    "standard_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# --- MMR Retriever ---\n",
    "mmr_retriever = vectorstore.as_retriever(\n",
    "    # To enable MMR, we set the search_type to \"mmr\".\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\n",
    "        \"k\": 4,          # The final number of documents to return.\n",
    "        \"fetch_k\": 10,   # The number of documents to initially fetch for MMR to consider.\n",
    "        \"lambda_mult\": 0.5 # The diversity factor (0 for max diversity, 1 for max relevance).\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f99849-5e72-463d-82c8-7c858b1ab6b9",
   "metadata": {},
   "source": [
    "### 4. Comparing the Retrieval Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836eb965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our query.\n",
    "query = \"How does LangChain support agents and memory?\"\n",
    "\n",
    "# Retrieve with standard similarity search.\n",
    "standard_results = standard_retriever.invoke(query)\n",
    "\n",
    "# Retrieve with MMR search.\n",
    "mmr_results = mmr_retriever.invoke(query)\n",
    "\n",
    "print(\"--- STANDARD SIMILARITY RESULTS ---\")\n",
    "for i, doc in enumerate(standard_results):\n",
    "    print(f\"{i+1}. {doc.page_content}\\n\")\n",
    "\n",
    "print(\"\\n--- MMR RESULTS ---\")\n",
    "for i, doc in enumerate(mmr_results):\n",
    "    print(f\"{i+1}. {doc.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a23eff-a1b2-c3d4-e5f6-a7b8c9d0e1f2",
   "metadata": {},
   "source": [
    "### 5. Building the RAG Pipeline with the MMR Retriever\n",
    "Now, we can simply plug our `mmr_retriever` into our standard RAG chain to leverage its diverse results for answer generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a54fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prompt and LLM Setup ---\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on the context provided.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "llm = init_chat_model(\"groq:gemma2-9b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cff3768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- RAG Pipeline with MMR Retriever ---\n",
    "document_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
    "# We use our `mmr_retriever` here.\n",
    "rag_chain = create_retrieval_chain(retriever=mmr_retriever, combine_docs_chain=document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ecccc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Query the RAG Chain ---\n",
    "response = rag_chain.invoke({\"input\": query})\n",
    "\n",
    "print(\"âœ… Final Answer:\\n\", response[\"answer\"])\n",
    "print(\"\\nðŸ“š Context Used:\")\n",
    "for doc in response['context']:\n",
    "    print(f\"- {doc.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f280a82b-5813-4c90-99c5-0370483ba645",
   "metadata": {},
   "source": [
    "### ðŸ”‘ Key Takeaways\n",
    "\n",
    "* **MMR Fights Redundancy**: The primary goal of Maximal Marginal Relevance (MMR) is to reduce redundancy in search results and increase the diversity of the retrieved documents.\n",
    "* **Balances Relevance and Diversity**: MMR works by iteratively selecting documents that are both relevant to the query and different from the documents already selected, providing a more comprehensive context.\n",
    "* **Easy to Implement in LangChain**: You can enable MMR in most vector store retrievers by simply setting `search_type=\"mmr\"`.\n",
    "* **Key Parameters to Tune**: You can control MMR's behavior with `k` (the final number of documents), `fetch_k` (the initial number of candidates to consider), and `lambda_mult` (the diversity factor).\n",
    "* **Improves Answer Quality**: By providing the LLM with a broader, less repetitive set of facts, MMR can lead to more complete and insightful answers in a RAG system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ultimate RAG Bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
