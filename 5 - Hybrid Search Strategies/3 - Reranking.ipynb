{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a987e0f2-b8d4-4b52-9c9e-5b23d9a1012a",
   "metadata": {},
   "source": [
    "### ðŸ“– Where We Are\n",
    "\n",
    "**In the last notebook**, we explored **Hybrid Search**, a technique that combines dense (semantic) and sparse (keyword) retrievers to improve recall and get a more robust set of initial documents.\n",
    "\n",
    "**In this notebook**, we'll learn how to take our retrieved results and make them even better with **Reranking**. Reranking is a second-stage process where a more powerful model scrutinizes the initial list of documents to re-order them based on true relevance. This adds a layer of intelligence to our RAG pipeline, ensuring the final LLM gets the highest possible quality context to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a23eff",
   "metadata": {},
   "source": [
    "### 1. Reranking with an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff5fac5",
   "metadata": {},
   "source": [
    "Reranking is a second-stage filtering process in retrieval systems. The typical workflow is:\n",
    "\n",
    "1.  **Retrieve**: Use a fast, efficient retriever (like FAISS or a hybrid retriever) to fetch an initial, larger set of top-k documents (e.g., k=10).\n",
    "2.  **Rerank**: Use a more accurate but slower model (like a powerful LLM or a cross-encoder) to re-score and reorder those 10 documents based on their relevance to the specific query.\n",
    "3.  **Generate**: Pass the final, re-ordered top-k documents (e.g., the best 3 out of the original 10) to the LLM for answer generation.\n",
    "\n",
    "ðŸ‘‰ This ensures that the most relevant documents appear at the top, significantly improving the quality of the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f5fd53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard LangChain Imports ---\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdf8243d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Load and Split Documents ---\n",
    "# Load a sample text file containing information about LangChain.\n",
    "loader = TextLoader(\"langchain_sample.txt\")\n",
    "raw_docs = loader.load()\n",
    "\n",
    "# Split the text into smaller, manageable document chunks.\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = splitter.split_documents(raw_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0b203af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the user query we will be working with.\n",
    "query = \"How can I use LangChain to build an application with memory and tools?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9720bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\Ultimate RAG Bootcamp\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Initial Retrieval ---\n",
    "# We'll set up a standard FAISS vector store retriever.\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(docs, embedding_model)\n",
    "\n",
    "# Note: We retrieve a larger number of documents (k=6) than we might actually need.\n",
    "# This gives the reranker more candidates to choose from, improving the final selection.\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cd8773a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Reranking with an LLM ---\n",
    "# Initialize the LLM that will act as our reranker.\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "llm = init_chat_model(\"groq:openai/gpt-oss-20b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2b0c172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use prompt engineering to instruct the LLM to perform the reranking task.\n",
    "# The prompt provides the query and the list of retrieved documents.\n",
    "# It explicitly asks the LLM to return only a comma-separated list of the document indices in their new order of relevance.\n",
    "rerank_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant. Your task is to rank the following documents from most to least relevant to the user's question.\n",
    "\n",
    "User Question: \"{question}\"\n",
    "\n",
    "Documents:\n",
    "{documents}\n",
    "\n",
    "Instructions:\n",
    "- Carefully evaluate the relevance of each document to the user's question.\n",
    "- Return a list of document indices in their new ranked order, from most to least relevant.\n",
    "- The output should be a comma-separated list of numbers (e.g., 2,1,3,0).\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25bf86b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the reranking chain using LCEL.\n",
    "reranker_chain = rerank_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3278ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initially Retrieved Documents (in order) ---\n",
      "0. LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\n",
      "Memory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.\n",
      "1. LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.\n",
      "2. LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.\n",
      "3. FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\n",
      "Agents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.\n",
      "4. Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\n",
      "BM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.\n",
      "5. Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.\n",
      "LangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.\n"
     ]
    }
   ],
   "source": [
    "# First, perform the initial retrieval.\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "# Format the retrieved documents to be passed into the reranking prompt.\n",
    "# We enumerate them so the LLM can refer to them by index.\n",
    "doc_list = [f\"{i}. {doc.page_content}\" for i, doc in enumerate(retrieved_docs)]\n",
    "formatted_docs = \"\\n\".join(doc_list)\n",
    "\n",
    "print(\"--- Initially Retrieved Documents (in order) ---\")\n",
    "print(formatted_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7e76228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM's Reranked Order: 0,1,3,2,4,5\n"
     ]
    }
   ],
   "source": [
    "# Invoke the reranker chain with the query and the formatted documents.\n",
    "reranked_order_str = reranker_chain.invoke({\"question\": query, \"documents\": formatted_docs})\n",
    "print(\"LLM's Reranked Order:\", reranked_order_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3c0699d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Final Reranked Documents ---\n",
      "\n",
      "Rank 1:\n",
      "LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\n",
      "Memory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.\n",
      "\n",
      "Rank 2:\n",
      "LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.\n",
      "\n",
      "Rank 3:\n",
      "FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\n",
      "Agents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.\n",
      "\n",
      "Rank 4:\n",
      "LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.\n",
      "\n",
      "Rank 5:\n",
      "Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\n",
      "BM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.\n",
      "\n",
      "Rank 6:\n",
      "Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.\n",
      "LangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.\n"
     ]
    }
   ],
   "source": [
    "# Parse the LLM's string output into a list of integer indices.\n",
    "# This can be a bit brittle, so more robust parsing might be needed in production.\n",
    "import re\n",
    "reranked_indices = [int(x.strip()) for x in re.findall(r'\\d+', reranked_order_str)]\n",
    "\n",
    "# Use the ranked indices to reorder our original list of retrieved documents.\n",
    "reranked_docs = [retrieved_docs[i] for i in reranked_indices if i < len(retrieved_docs)]\n",
    "\n",
    "print(\"--- Final Reranked Documents ---\")\n",
    "for i, doc in enumerate(reranked_docs, 1):\n",
    "    # This line has been corrected to form a valid f-string.\n",
    "    print(f\"\\nRank {i}:\\n{doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f280a82b-5813-4c90-99c5-0370483ba645",
   "metadata": {},
   "source": [
    "### ðŸ”‘ Key Takeaways\n",
    "\n",
    "* **Reranking Boosts Precision**: Reranking is a two-stage process (fast retrieval, then accurate re-scoring) that prioritizes the absolute most relevant documents, significantly improving the precision of your context.\n",
    "* **LLMs as Rerankers**: You can effectively use a powerful LLM as a sophisticated reranker. By providing the initial documents and a carefully crafted prompt, you can leverage the LLM's deep understanding to re-evaluate relevance.\n",
    "* **Retrieve More, Then Filter**: A common pattern is to retrieve more documents than you need in the first stage (e.g., top 10) and then use the reranker to select and order the final, most relevant set (e.g., top 3).\n",
    "* **Trade-offs**: This method adds an extra LLM call, which introduces a small amount of latency and cost. However, the resulting improvement in context quality often leads to better, faster, and more accurate final answers, justifying the trade-off."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ultimate RAG Bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
