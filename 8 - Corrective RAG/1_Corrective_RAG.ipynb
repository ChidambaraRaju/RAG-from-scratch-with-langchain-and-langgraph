{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a987e0f2-b8d4-4b52-9c9e-5b23d9a1012a",
   "metadata": {},
   "source": [
    "### üìñ Where We Are\n",
    "\n",
    "**In the last notebook**, we built a powerful **Agentic RAG** system that could intelligently choose between different tools. This introduced the idea of a dynamic, reasoning-driven workflow.\n",
    "\n",
    "**In this notebook**, we'll explore another advanced agentic pattern: **Corrective RAG (C-RAG)**. This architecture adds a crucial layer of self-correction to the retrieval process. The system will learn to grade the relevance of its retrieved documents and, if they are not good enough, take corrective actions like rewriting the query and falling back to a web search. This creates a highly robust and accurate RAG system that can recover from initial retrieval failures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b6ae34",
   "metadata": {},
   "source": [
    "### 1. Understanding Corrective RAG (C-RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b320d771-48d6-4448-9c4c-31d77a83d973",
   "metadata": {},
   "source": [
    "Corrective Retrieval-Augmented Generation (C-RAG) is an advanced, adaptive RAG pattern designed to make retrieval systems more robust and accurate. Its core innovation is a built-in **self-correction loop**. Instead of blindly trusting the documents it retrieves, a C-RAG system actively assesses the quality of those documents and takes corrective actions if they are irrelevant.\n",
    "\n",
    "#### The Problem: When Initial Retrieval Fails\n",
    "\n",
    "A standard RAG pipeline can fail if the initial document retrieval is poor, leading to hallucinations or \"I don't know\" answers. C-RAG adds a quality control gate to recover from this.\n",
    "\n",
    "**Analogy: The Expert Researcher with Internet Access üë©‚Äçüíª**\n",
    "\n",
    "-   A **Standard RAG System** is a researcher who can only look in the company's private library. If the answer isn't there, they write a poor report or give up.\n",
    "-   A **Corrective RAG System** is a more seasoned researcher:\n",
    "    1.  They first check the internal company library (**Retrieve**).\n",
    "    2.  They critically read the documents they found and ask, \"Does this actually answer my question?\" (**Grade**).\n",
    "    3.  **If the documents are good**, they write their report based on them (**Generate**).\n",
    "    4.  **If the documents are bad**, they think, \"The internal library is insufficient. I need to rephrase my search and look elsewhere.\" (**Transform Query**). Then, they use a public search engine (**Web Search**) to find the correct information before writing their final, well-sourced report (**Generate**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81a31e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Environment Setup ---\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() # Loading all the environment variables\n",
    "\n",
    "# Set API keys for the services we'll use.\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21b2f4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Build the Index (Primary Knowledge Base) ---\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Define the URLs for our knowledge base.\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "# Load, split, and index the documents into a FAISS vector store.\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=500, chunk_overlap=0)\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "vectorstore = FAISS.from_documents(documents=doc_splits, embedding=HuggingFaceEmbeddings(model=\"all-MiniLM-L6-v2\"))\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a2a468-18e3-4610-ad44-783ca89369d7",
   "metadata": {},
   "source": [
    "### 3. Building the C-RAG Components\n",
    "We will build our Corrective RAG system from modular components using standard LangChain chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7e36b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Component 1: The Retrieval Grader ---\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "    binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n",
    "\n",
    "llm = ChatGroq(model=\"llama-3.3-70b-versatile\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "system = \"\"\"You are a grader assessing the relevance of a retrieved document to a user question. \n",
    "    If the document contains keywords or semantic meaning related to the question, grade it as relevant. \n",
    "    Give a binary score 'yes' or 'no' to indicate whether the document is relevant.\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "])\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "000e602b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Desktop\\Ultimate RAG Bootcamp\\.venv\\Lib\\site-packages\\langsmith\\client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# --- Component 2: The Answer Generator ---\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "rag_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4fbfa607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Component 3: The Question Rewriter ---\n",
    "system = \"\"\"You are a question re-writer that converts an input question to a better version that is optimized for web search.\n",
    "     Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
    "re_write_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"human\", \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\"),\n",
    "])\n",
    "\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d72a121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Component 4: The Web Search Tool (Fallback) ---\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f99849-5e72-463d-82c8-7c858b1ab6b9",
   "metadata": {},
   "source": [
    "### 4. Implementing the Corrective RAG Logic\n",
    "Now, we'll create a main function that orchestrates the entire self-correcting loop using the components we just built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7fcdf895",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "def corrective_rag_flow(question: str):\n",
    "    \"\"\"Implements the full Corrective RAG workflow.\"\"\"\n",
    "    \n",
    "    print(\"--- STEP 1: INITIAL RETRIEVAL ---\")\n",
    "    documents = retriever.invoke(question)\n",
    "    print(f\"Retrieved {len(documents)} documents.\")\n",
    "    \n",
    "    print(\"\\n--- STEP 2: GRADING DOCUMENTS ---\")\n",
    "    filtered_docs = []\n",
    "    for doc in documents:\n",
    "        grade = retrieval_grader.invoke({\"question\": question, \"document\": doc.page_content})\n",
    "        if grade.binary_score == \"yes\":\n",
    "            print(f\"- Document RELEVANT\")\n",
    "            filtered_docs.append(doc)\n",
    "        else:\n",
    "            print(f\"- Document NOT RELEVANT\")\n",
    "    \n",
    "    # If all retrieved documents are irrelevant, we trigger the corrective actions.\n",
    "    if not filtered_docs:\n",
    "        print(\"\\n--- STEP 3a: CORRECTIVE ACTION - REWRITING QUERY ---\")\n",
    "        new_question = question_rewriter.invoke({\"question\": question})\n",
    "        print(\"Rewritten Question:\", new_question)\n",
    "        \n",
    "        print(\"\\n--- STEP 3b: CORRECTIVE ACTION - WEB SEARCH ---\")\n",
    "        # The error was here. The Tavily tool returns a list of dictionaries.\n",
    "        # We need to access the 'content' key from each dictionary.\n",
    "        web_search_results = web_search_tool.invoke({\"query\": new_question})\n",
    "        web_content = \"\\n\".join([d[\"content\"] for d in web_search_results])\n",
    "        \n",
    "        # Add the web search results as a new document to our context.\n",
    "        filtered_docs.append(Document(page_content=web_content))\n",
    "    \n",
    "    print(\"\\n--- STEP 4: GENERATING FINAL ANSWER ---\")\n",
    "    final_answer = rag_chain.invoke({\"context\": filtered_docs, \"question\": question})\n",
    "    \n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b9ea1ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STEP 1: INITIAL RETRIEVAL ---\n",
      "Retrieved 4 documents.\n",
      "\n",
      "--- STEP 2: GRADING DOCUMENTS ---\n",
      "- Document NOT RELEVANT\n",
      "- Document RELEVANT\n",
      "- Document RELEVANT\n",
      "- Document NOT RELEVANT\n",
      "\n",
      "--- STEP 4: GENERATING FINAL ANSWER ---\n",
      "\n",
      "--- FINAL ANSWER ---\n",
      "The types of agent memory are short-term memory and long-term memory. Short-term memory utilizes in-context learning, while long-term memory provides the capability to retain and recall information over extended periods. This is achieved by leveraging an external vector store and fast retrieval.\n"
     ]
    }
   ],
   "source": [
    "# Run the graph with a sample question.\n",
    "final_answer = corrective_rag_flow(\"What are the types of agent memory?\")\n",
    "print(\"\\n--- FINAL ANSWER ---\")\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f280a82b-5813-4c90-9c5-0370483ba645",
   "metadata": {},
   "source": [
    "### üîë Key Takeaways\n",
    "\n",
    "* **C-RAG is a Robust Pattern**: Corrective RAG builds a self-correction loop into your retrieval process, making it more resilient to initial retrieval failures.\n",
    "* **Grading is the Quality Gate**: The core of C-RAG is the **grading** step, where an LLM assesses the relevance of retrieved documents. This allows the system to identify when it needs to take corrective action.\n",
    "* **Correction through Rewriting and Fallbacks**: When retrieval fails, the system can **rewrite** the query for better clarity and/or fall back to an external knowledge source like a **web search** to find the necessary information.\n",
    "* **Modular Implementation**: You can implement this pattern by creating separate, modular components (grader, rewriter, generator) and orchestrating them with a controlling function or a graph framework like LangGraph."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ultimate RAG Bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
